{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from load_data import load_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.nn import depth_to_space\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_blocks = 5\n",
    "downsample_factor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = r\"C:\\Users\\nedst\\Desktop\\synoptic-project-NedStickler\\.venv\\Lib\\site-packages\\tensorflow_datasets\"\n",
    "dataset, _ = load_data(package_path)\n",
    "dataset = dataset[:2048, :, :, :]\n",
    "lr_dataset = np.array([image[::downsample_factor, ::downsample_factor, :] for image in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelShuffle(keras.Layer):\n",
    "    def call(self, x):\n",
    "        return depth_to_space(x, 2)\n",
    "\n",
    "def d_residual_block(x, n_filters, n_strides):\n",
    "    x = layers.Conv2D(n_filters, kernel_size=3, strides=n_strides, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    return x\n",
    "\n",
    "def d_downsample_pair(x, n_filters):\n",
    "    x = d_residual_block(x, n_filters, 1)\n",
    "    x = d_residual_block(x, n_filters, 2)\n",
    "    return x\n",
    "\n",
    "def g_residual_block(x_in):\n",
    "    x = layers.Conv2D(64, kernel_size=3, padding=\"same\")(x_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    x = layers.Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "def discriminator():\n",
    "    # HR/SR input\n",
    "    inputs = layers.Input((None, None, 3))\n",
    "    x = layers.Rescaling(scale=1/127.5, offset=-1)(inputs)\n",
    "    \n",
    "    # First convolution blocks\n",
    "    x = layers.Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # Residual downsampling blocks\n",
    "    x = d_residual_block(x, 64, 2)\n",
    "    x = d_downsample_pair(x, 128)\n",
    "    x = d_downsample_pair(x, 256)\n",
    "    x = d_downsample_pair(x, 512)\n",
    "\n",
    "    # Flatten and classify\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return keras.Model(inputs, x)\n",
    "\n",
    "\n",
    "def generator(residual_blocks):\n",
    "    # LR input\n",
    "    inputs = layers.Input((None, None, 3))\n",
    "    x_in = layers.Rescaling(scale=1/255)(inputs)\n",
    "\n",
    "    # First convolution\n",
    "    x_in = layers.Conv2D(64, kernel_size=3, padding=\"same\")(x_in)\n",
    "    x_in = x = layers.PReLU(shared_axes=[1, 2])(x_in)\n",
    "\n",
    "    # Residual block set\n",
    "    for _ in range(residual_blocks):\n",
    "        x = g_residual_block(x)\n",
    "    \n",
    "    # Residual block without activation functions\n",
    "    x = layers.Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x_in, x])\n",
    "\n",
    "    # Upscaling blocks\n",
    "    x = layers.Conv2D(256, kernel_size=3, padding=\"same\")(x)\n",
    "    x = PixelShuffle()(x)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, kernel_size=3, padding=\"same\")(x)\n",
    "    x = PixelShuffle()(x)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    # Final convolve\n",
    "    x = layers.Conv2D(3, kernel_size=3, padding=\"same\")(x)\n",
    "    x = layers.Rescaling(scale=127.5, offset=127.5)(x)\n",
    "\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, vgg):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.vgg = vgg\n",
    "    \n",
    "    def compile(self, d_optimiser, g_optimiser, bce_loss, mse_loss):\n",
    "        super().compile()\n",
    "        self.d_optimiser = d_optimiser\n",
    "        self.g_optimiser = g_optimiser\n",
    "        self.bce_loss = bce_loss\n",
    "        self.mse_loss = mse_loss\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        lr_images, hr_images = data\n",
    "        batch_size = lr_images.shape[0]\n",
    "\n",
    "        # Train the discriminator\n",
    "        generated_images = self.generator(lr_images)\n",
    "        combined_images = keras.ops.concatenate([generated_images, hr_images])\n",
    "        d_labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.bce_loss(d_labels, predictions)\n",
    "        gradients = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimiser.apply_gradients(zip(gradients, self.discriminator.trainable_weights))\n",
    "\n",
    "        # Train the generator\n",
    "        misleading_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator(lr_images)\n",
    "            predictions = self.discriminator(generated_images)\n",
    "            # continue\n",
    "\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m srgan \u001b[38;5;241m=\u001b[39m SRGAN(discriminator\u001b[38;5;241m=\u001b[39mdiscriminator(), generator\u001b[38;5;241m=\u001b[39mgenerator(residual_blocks), vgg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m srgan\u001b[38;5;241m.\u001b[39mcompile(d_optimiser\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0003\u001b[39m), g_optimiser\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0003\u001b[39m), bce_loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(), mse_loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError())\n\u001b[1;32m----> 4\u001b[0m \u001b[43msrgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nedst\\Desktop\\synoptic-project-NedStickler\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\nedst\\Desktop\\synoptic-project-NedStickler\\.venv\\lib\\site-packages\\keras\\src\\trainers\\trainer.py:854\u001b[0m, in \u001b[0;36mTrainer._pythonify_logs\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pythonify_logs\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs):\n\u001b[0;32m    853\u001b[0m     result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 854\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()):\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    856\u001b[0m             result\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(value))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "srgan = SRGAN(discriminator=discriminator(), generator=generator(residual_blocks), vgg=None)\n",
    "srgan.compile(d_optimiser=keras.optimizers.Adam(learning_rate=0.0003), g_optimiser=keras.optimizers.Adam(learning_rate=0.0003), bce_loss=keras.losses.BinaryCrossentropy(), mse_loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "srgan.fit(lr_dataset, dataset, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
