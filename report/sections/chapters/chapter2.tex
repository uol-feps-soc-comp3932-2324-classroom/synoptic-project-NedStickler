\chapter{Methods}
\label{chapter2}

\section{Tools}

\section{Data}
Learning-based models require data to learn from. In the case of super-resolution, the data consists of pairs of images: a high-resolution and low-resolution version of each training instance, where the model will attempt to learn some `mapping' from LR to HR.\@ The learnt mapping can then be applied to unseen low-resolution imagery where a super-resolution representation is generated. Thus, the relevance and quality of dataset contents is monumental in producing a successful SR reconstruction model.\ \textcolor{blue}{Definitely needs more!}

\subsection{Datasets}
A series of image datasets have been widely accepted by the community as effective at training learning-based super-resolution reconstruction methods. Wang et al.~\cite{remoteSensingDeepLearningReview, remoteSensingGANsReview} overview such datasets in two papers exploring learning based-solutions. Both remote sensing and non-remote sensing datasets are visited. As the goal of this report is to identify the effectiveness of the SRGAN model when applied to remote sensing data, our training dataset must be composed entirely of remote sensing images. Otherwise, the performance metrics we collect after model training will not accurately capture the applicability of the SRGAN model to remote sensing imagery.\ \textcolor{blue}{Needs expanding!} Consequently, we can focus our attention towards the remote sensing image datasets provided by Wang et al. and ignore the general image datasets.

\begin{table}
    \centering
    \begin{tabular}{|cccc|}
        \hline
        \textbf{Name} & \textbf{Size} & \textbf{Resolution} & \textbf{File type} \\
        \hline
        AID & 10000 & 600 $\times$ 600 & JPG \\
        RSSCN7 & 2800 & 400 $\times$ 400 & JPG \\
        WHU-RS19 & 1005 & 600 $\times$ 600 & TIF \\
        UC Merced & 2100 & 256 $\times$ 256 & PNG \\
        NWPU-RESISC45 & 31500 & 256 $\times$ 256 & PNG \\
        RSC11 & 1232 & 512 $\times$ 512 & TIF \\
        UCAS-AOD & 910 & 1280 $\times$ 659 & PNG \\
        SIRI-WHU & 2400 & 200 $\times$ 200 & TIF \\
        ITCUD & 135 & 5616 $\times$ 3744 & JPG \\
        DIOR & 23463 & 800 $\times$ 800 & JPG \\
        DOTA & 2806 & 800 $\times$ 4000 & PNG \\
        \hline
    \end{tabular}
    \caption{Table of remote sensing datasets commonly used for SR reconstruction as suggested by Wang et al.~\cite{remoteSensingDeepLearningReview,remoteSensingGANsReview}.}
    \label{table:datasets_table}
\end{table}

Table~\ref{table:datasets_table} shows the remote sensing imagery datasets as listed by Wang et al.\ along with the size of the dataset, the resolution of each image, and the image file type. Selecting an approriate dataset for model training is crucial for producing good SR reconstructions~\cite{ref}. Ideally the training dataset would be as large and diverse as possible, but due to hardware constraints (see more in section [training section]) training time and memory resources are limited. To avoid large training times the training data should be limited to between 1000 to 3000 images, with an image resolution of less than 1000 $\times$ 1000.\ \textcolor{blue}{Need to prove why this is the case.} Additionally, there must be a enough images to allow for test and validation sets for analysing model performance.

Following these requirements removes the WHU-RS19, UCAS-AOD, and ITCUD datasets for having too few images, along with DOTA for having too large of a resolution. This yields the AID, RSSCN7, UC Merced, NWPU-RESISC45, RSC11, SIRI-WHU and DIOR datasets. Any of these datasets would prove effective for training the SRGAN model. Therefore, selection of a training dataset is based off personal preference. 

Notably, the NWPU-RESISC45 dataset contains the most images with the most image classes, meaning more opportunity for the model to capture nuances within categories of remote sensing imagery and produce better SR results~\cite{ref}. The dataset is also accompanied by a pre-built data loader as a part of the Python libary tensorflow-datasets~\cite{ref}. This makes loading and processing the data convenient and efficient. With the large size of the dataset, selecting subsets of images for training, test and validation becomes easy.\ \textcolor{blue}{Need to mention why the others are a bad choice.} Note that the imposed hardware requirements renders training with the entire dataset infeasible. For these reasons the SRGAN models proposed in this project will be trained and evaluated using the NWPU-RESISC45 dataset.

\subsection{NWPU-RESISC45 dataset}
The Northwestern Polytechnical University REmote Sensing Image Scene Classification dataset consists of 31500 remote sensing images gathered from Google Earth~\cite{resisc45}. 

\subsection{Data preparation}
The data we use to train machine learning models must be correctly prepared to ensure efficient training and good results~\cite{ref}. The degree of preparation required greatly depends on the type of data the model will learn from. In the case of this project, our model learns from image data.

\section{Model design}

\subsection{Novel feature}

\section{Training}

\subsection{Hardware}
GPUs, university machines, restarts.

\subsection{Existing guidance}
\textcolor{blue}{Will need to move lots of the following section into here.}

\subsection{Procedure}
\textcolor{blue}{Still need to talk about model training prior to selecting dataset and reference!}\ The SRGAN model was trained in accordance with the training details and guidance provided by Ledig et al.~\cite{srgan} with minor deviations to account for encountered limitations. Firstly, the SRGAN model was trained on the subset of the NWPU-RESISC45 dataset as explained in [put section here] instead of the ImageNet database subset of size $n \approx 350000$. The ImageNet database contains a far higher inter- and intra-class variance due to the dataset being composed of many image categories, not just remote sensing imagery\ \textcolor{blue}{Not sure if needed here or in a previous section.} The training and test image sets are completely distinct from eachother to accurately evaluate the effectiveness of the model when applied to new images.\ \textcolor{blue}{Hmm not sure.} The LR images were generated with the same method used by Ledig et al., where they used a bicubic kernel with a downsample factor $r=4$. This achieves the affect of reducing the image resolution by a factor of 4. We use the Adam optimiser~\cite{ref} with $\beta_1 = 0.9$. Prior to SRGAN training, the SRResNet models were pretrained to help avoid unwanted local optima in adversarial training. In the guidance Ledig et al.\ use $10^6$ update iterations with a learning rate of $10^{-4}$ to pretrain the SRResNet model. Training the model on hardware accessible for the during of this project, with the selected dataset, batch size and image patches would take $~80$ hours. Training on University of Leeds machines requires training to be completed within less than 24 hours due to the daily restart that the machines undergo. Therefore, for this project it was deemed necessary to reduce the number of update iterations to save time, avoid failing training cycles and keep the pace of the project up. It is still important to preserve as much of the original training advice as possible to effectively compare the difficulties the SRGAN model encounters we applied to remote sensing imagery. Therefore, the number of update iterations was reduced by a factor of 10 to $10^5$, yielding a training time of just over 8 hours.

Following the successful pretraining of the SRResNet model various SRGAN models were trained as shown in [figure]. The SRGAN models were trained twice, firstly with a learning rate of $10^{-4}$, then again for the same number of update iterations but with a learning rate of $10^{-5}$, effectively working as `fine tuning' training~\cite{srgan}. Ledig et al.\ train their model for $10^5$ update iterations, but due to the time-saving reduction employed for this project we opted to reduce this number to $10^4$ update iterations. This falls in line with the factor of 10 decrease in update iterations for the SRResNet pretraining.

When training both the SRResNet and SRGAN models, patches are taken from each training image in each batch. 16 random $96 \times 96$ sized patches are taken from the each image instance. These act as the HR image inputs. The downscaled LR representations are taken from these image patches using the bicubic downsampling kernel [as described earlier]. This results in 240 unique image pairs for training for each update iteration (15 images per batch, 16 patches per image). It was intended for 16 image patches to be taken from each training image in both the SRResNet and SRGAN models, however due to the enhanced memory requirements of the SRGAN model we could not generate 240 patches and the program threw out of memory errors. To remediate this issue we reduced the number of patches taken per training instance until we no longer received out of memory errors. By systemaically testing less and less patches per image we arrived at $n=10$ patches per image, resulting in 150 image pairs per update iteration. This is only applicable for the SRGAN model training and the SRResNet model was trained with 16 image patches as orginally intended. This issue was unavoidable on the University machines due to amount of RAM available to the GPU in training.\ \textcolor{blue}{Describe why enhanced memory requirements.} 
 
<Everything that comes under the `Methods' criterion in the mark scheme should be described in one, or possibly more than one, chapter(s).>

<Methods requirements. Is it clear that\dots
\begin{itemize}
    \item The solution exists?
    \item I have produced the deliverables specified?
    \item Appropriate steps or standards were taken to ensure a quality output?
    \item The challenges were clearly articulated?
\end{itemize}
>
