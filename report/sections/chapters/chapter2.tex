\chapter{Methods}\label{chapter2}
\section{Tools}
Python, Keras, Matplotlib, numpy, tensorflow

\section{Data}
Learning-based models require data to learn from. In the case of super-resolution, the data consists of pairs of images: a high-resolution and low-resolution version of each training instance, where the model will attempt to learn some `mapping' from LR to HR.\@ The learned mapping can then be applied to unseen low-resolution imagery where a super-resolution representation is generated. Thus, the relevance and quality of dataset contents is monumental in producing a successful SR reconstruction model.\ \textcolor{blue}{Definitely needs more!}

\subsection{Datasets}
A series of image datasets have been widely accepted by the community as effective at training learning-based super-resolution reconstruction methods. Wang et al.~\cite{remoteSensingDeepLearningReview, remoteSensingGANsReview} overview such datasets in two papers exploring learning based-solutions. Both remote sensing and non-remote sensing datasets are visited. As the goal of this report is to identify the effectiveness of the SRGAN model when applied to remote sensing data, our training dataset must be composed entirely of remote sensing images. Otherwise, the performance metrics we collect after model training will not accurately capture the applicability of the SRGAN model to remote sensing imagery.\ \textcolor{blue}{Needs expanding!} Consequently, we can focus our attention towards the remote sensing image datasets provided by Wang et al. and ignore the general image datasets.
\begin{table}
    \centering
    \begin{tabular}{cccc}
        \toprule
        \textbf{Name} & \textbf{Size} & \textbf{Resolution} & \textbf{File type} \\
        \midrule
        AID & 10000 & 600 $\times$ 600 & JPG \\
        RSSCN7 & 2800 & 400 $\times$ 400 & JPG \\
        WHU-RS19 & 1005 & 600 $\times$ 600 & TIF \\
        UC Merced & 2100 & 256 $\times$ 256 & PNG \\
        NWPU-RESISC45 & 31500 & 256 $\times$ 256 & PNG \\
        RSC11 & 1232 & 512 $\times$ 512 & TIF \\
        UCAS-AOD & 910 & 1280 $\times$ 659 & PNG \\
        SIRI-WHU & 2400 & 200 $\times$ 200 & TIF \\
        ITCUD & 135 & 5616 $\times$ 3744 & JPG \\
        DIOR & 23463 & 800 $\times$ 800 & JPG \\
        DOTA & 2806 & 800 $\times$ 4000 & PNG \\
        \bottomrule
    \end{tabular}
    \caption{Table of remote sensing datasets commonly used for SR reconstruction as suggested by Wang et al.~\cite{remoteSensingDeepLearningReview,remoteSensingGANsReview}.}
    \label{table:datasets_table}
\end{table}
Table~\ref{table:datasets_table} shows the remote sensing imagery datasets as listed by Wang et al.\ along with the size of the dataset, the resolution of each image, and the image file type. Selecting an appropriate dataset for model training is crucial for producing good SR reconstructions~\cite{ref}. Ideally the training dataset would be as large and diverse as possible, but due to hardware constraints (see more in section [training section]) training time and memory resources are limited. To avoid large training times the training data should be limited to between 1000 and 3000 images, with an image resolution of less than 1000 $\times$ 1000.\ \textcolor{blue}{Need to prove why this is the case.} Additionally, there must be enough images to allow for test and validation sets for analysing model performance.

Following these requirements removes the WHU-RS19, UCAS-AOD, and ITCUD datasets for having too few images, along with DOTA for having too large of a resolution. This yields the AID, RSSCN7, UC Merced, NWPU-RESISC45, RSC11, SIRI-WHU and DIOR datasets. Any of these datasets would prove effective for training the SRGAN model. Therefore, selection of a training dataset is based off personal preference. 

Notably, the NWPU-RESISC45 dataset contains the most images with the most image classes, meaning more opportunity for the model to capture nuances within categories of remote sensing imagery and produce better SR results~\cite{ref}. The dataset is also accompanied by a pre-built data loader as a part of the Python libary tensorflow-datasets~\cite{ref}. This makes loading and processing the data convenient and efficient. With the large size of the dataset, selecting subsets of images for training, test and validation becomes easy.\ \textcolor{blue}{Need to mention why the others are a bad choice.} Note that the imposed hardware requirements renders training with the entire dataset infeasible. For these reasons the SRGAN models proposed in this project will be trained and evaluated using the NWPU-RESISC45 dataset.

\subsection{NWPU-RESISC45 dataset}\label{subsec:resisc45}
The Northwestern Polytechnical University REmote Sensing Image Scene Classification dataset consists of 31500 remote sensing images gathered from Google Earth~\cite{resisc45}. 

\subsection{Data preparation}\label{subsec:data_preparation}
The data we use to train machine learning models must be correctly prepared to ensure efficient training and good results~\cite{ref}. The degree of preparation required greatly depends on the type of data the model will learn from. In the case of this project, our model learns from image data. Images also blurred before downsampling operation to stop artefacts from being introduced into downscaled images due to bicubic interpolation.

\section{Solution}
In the following section we provide a solution to the SR reconstruction problem in remote sensing. We adapt the loss function of a pre-existing model by systematically testing model performance with different loss variations. Details of the implementation are presented.

\subsection{Architecture}
The architecture of the solution is provided by the SRGAN model proposed by Ledig et al.\ in 2017. SRGAN was the first generative adversarial network-based architecture proposed as a solution to the SR reconstruction problem. Being the first of its kind, using the SRGAN architecture provides a well-established foundation for development, and as a result we were able to implement the model at pace whilst being partially aided by pre-existing external sources~\cite{ref}. Implementing SRGAN from scratch allowed us to customise model architecture, training and validation processes, and loss components without encountering difficulties often associated with using existing implementations~\cite{ref}. Additionally, SRGAN is relatively simple when compared to more recent SR reconstruction solutions. This aligned with our project goals as constraints imposed by training hardware limited our ability to employ especially complex models with many parameters.

The SRGAN architecture follows the typical generative adversarial network architecture~\cite{ref} and consists of both a generator and discriminator. The role of the generator is to produce SR reconstructions, and the discriminator aids in guiding the generator towards producing a desirable result through adversarial training.

Generator architecture.

The discriminator follows a standard binary classifier architecture. Images are passed through several convolutional blocks that capture the image features and then downsample. Following these blocks is a fully connected layer that reduces the number of neurons down to one, where the activation of the final neuron decides the classification of the image. In the case of a GAN, this classification is either real or fake. HR ground-truth images should be classified as real, and SR reconstructions should be classified as fake. The binary cross-entropy loss is calculated between the actual and predicted image labels to give a value representing the `distance' the discriminator needs to cross to achieve perfect classification. The discriminator steps towards a lower loss value to produce a better classification result.

\subsection{Training}

\subsection{Improving loss}
The SRGAN model uses a perceptual loss component to learn the mappings between LR and HR imagery. Ledig et al.\ utilise a pretrained VGG19 model to produce feature maps for the both the generated SR image and the HR ground truth. The pixel-wise mean squared error is calculated to provide a scalar value representing the perceptual difference between the two images, which acts as a component of the training loss. The VGG19 model is trained to classify images, so the learned feature maps produce the highest classification accuracy possible. To produce a good classification accuracy the feature maps must identify the most important and distinctive features of each class. By calculating the difference between these feature maps we can see how much the most important features in the SR reconstruction differ from the ground-truth image, and point the model towards a result with greater realism.

We can extend this idea to image classification models that outperform VGG19. A superior classification accuracy suggests the model has better extracted the most important image features~\cite{ref}. We can utilise such models as replacements for the VGG19 component of the perceptual loss. We offer the following conjecture: `using the feature maps from more accurate image classification models in the perceptual loss component of SRGAN-based models will increase SR reconstruction performance'.

To explore this, with the aim of producing a solution to the SR reconstruction in remote sensing, we  test model performance using feature maps from a variety of pretrained image classification models. The Keras library provides numerous image classifiers pretrained on the ImageNet database. Within the documentation for these pretrained classifiers are accuracy metrics calculated on the ImageNet validation dataset, allowing us to select models based on their performance. The available classifiers can be grouped into classes based on their model architecture. The model with the highest accuracy from each class was selected for testing as the perceptual loss component for SRGAN training. A full list of available models can be found in Appendix~\ref{appendix}.

Calculating the loss with a pretrained model requires a minor alteration to the model architecture to be feasible. The models consist of convolutional sections followed by fully-connected layers of neurons, where the activations of the final layer of neurons represent the `certainty' that the image belongs to that class. We are not interested in the classification of our images, but rather the distinct image features, thus we remove the fully connected layers of neurons from each model leaving the desired feature maps. When calculating loss during training time, both the HR image and the SR reconstruction are passed through the model and the image feature maps are created. The pixel wise mean squared error is calculated between the feature maps, and the resulting value is the loss for that training step. Minimising this value produces better SR results.

\begin{table}
    \centering
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Classifier} & \textbf{Top-1 accuracy} & \textbf{Top-5 accuracy} & \textbf{Size (MB)} & \textbf{Parameters (M)} \\
        \midrule
        Xception & 79.0\% & 94.5\% & 88 & 22.9 \\
        ResNet152V2 & 77.2\% & 94.2\% & 232 & 60.4 \\
        InceptionV3 & 77.9\% & 93.7\% & 92 & 23.9 \\
        InceptionResNetV2 & 80.3 \% & 95.3\% & 215 & 55.9 \\
        MobileNetV2 & 71.3\% & 90.1\% & \colorbox{YellowGreen}{12} & \colorbox{YellowGreen}{3.5} \\
        DenseNet201 & 77.3\% & 93.6\% & 80 & 20.2 \\
        NASNetLarge & 82.5\% & 96.0\% & 343 & 88.9 \\
    EfficientNetV2L & \colorbox{YellowGreen}{85.7\%} & \colorbox{YellowGreen}{97.5\%} & 479 & 119.0 \\
        \bottomrule
    \end{tabular}
    \caption{List of pretrained image classifiers tested as the perceptual loss component for SRGAN training. The best value in each category is highlighted in green.}
    \label{table:pretrained_classifiers}
\end{table}

For each of the pretrained classifiers an SRGAN model was trained using the classifier feature maps to calculate the perceptual loss. The training procedure used for the SRGAN-based models is described in section~\ref{subsec:procedure}.

\subsection{Implementation}

\section{Training}

\subsection{Hardware}
GPUs, university machines, restarts.

\subsection{Existing guidance}
Ledig et al.~\cite{srgan} provide in-depth detail on how they trained the SRResNet and SRGAN models to achieve high-quality SR results. Both models were trained on a 350000 image subset of the ImageNet database~\cite{ref}. 16 sub-image patches of size $96 \times 96$ are taken of distinct training images. The Adam~\cite{ref} optimiser is used with $\beta_1 = 0.9$. The SRResNet model is trained on the dataset for a total of $10^6$ update iterations using a learning rate of $10^{-4}$. The SRGAN model is trained for $10^5$ update iterations with an initial learning rate of $10^{-4}$, followed by another $10^5$ update iterations using a learning rate of $10^{-5}$. To avoid unwanted local optima the pretrained SRResNet is used as the generator for the SRGAN model.

\subsection{Procedure}\label{subsec:procedure}
Training the SRResNet and SRGAN models from scratch will require a deviation from existing training guidance to account for time and hardware limitations. This section outlines the exact training procedure that was followed along with justification for the differences to the original procedure as defined by Ledig et al.~\cite{srgan}.

The guidance regarding the randomly cropped sub-images used to train the model is unclear. The exact guidance states `For each mini-batch we crop 16 random $96 \times 96$ HR sub images of distinct training images'. This could be interpreted in two ways: either a batch consists of 16 images and a single $96 \times 96$ patch is taken from each image in the batch, or, a batch has an undefined size, but for every image in the batch 16 random patches of size $96 \times 96$ are taken to increase the training set size, known as data augmentation~\cite{ref}. For the purposes of this project we assume the second definition to be true as it allows us to introduce more training examples to the model through data augmentation to hopefully improve final performance\ \textcolor{blue}{Needs more I think. Could also perform a test where we use batches and no batches and see the difference.}

Taking the sub-images definition as described above requires us to choose an appropriate batch size. Choosing a batch size requires balancing gradient smoothness and memory constraints~\cite{ref}. A larger batch size produces a smoother gradient but also requires significantly more memory and computation resources. Alongside this, we also need to consider a batch size that nicely factors into our dataset size. Picking a dataset size, batch size and number of patches required an investigation into the memory limitations introduced by the training hardware. The number of image classes in the NWPU-RESISC45 dataset is considered first. There are 45 distinct image classes, as listed in section~\ref{subsec:resisc45}. To avoid unintentionally overfitting our SR reconstruction model to a specific class we must include an equal number of images from each of the 45 classes, consequently requiring our dataset size to be a multiple of 45. Therefore, it is also required that the batch size is a multiple or factor of 45 to ensure that the entire dataset is visited over a single epoch and no training examples are skipped due to division remainders. As mentioned earlier, a larger batch size produces a smoother gradient and therefore more stable training, however the hardware available restricts how large the batch size can be. Employing a method of trial and error identifies a batch size suitable for training the SR reconstruction model whilst adhering to memory constraints. A sample dataset of size 450 is used to test different batch sizes, with the intention of later increasing the dataset size to increase training examples. Firstly, training of the SRGAN model begins with a batch size and a single image patch per training example. The system attempts to allocate the required memory resources and computing power from the GPU but is unable to, resulting in an out-of-memory (OOM) exception. The batch size is then reduced, and the process is repeated until no OOM exception occurs. In practice the first batch size selected was $b = 30$, which resulted in an OOM exception. This value was reduced until a final batch size of $b = 15$ was selected. The next step was to test the memory constraints on the number of patches taken from each training example. A similar approach was taken to selecting batch size, where the number of patches was reduced until running model training no longer yielded an OOM exception. The initial value was $p = 16$, as suggested by Ledig et al., with the test yielding $p = 10$ patches per training instance. Further testing showed that for $p = 10$ and $p = 9$ that training would begin but that halt at seemingly random points in the process. To combat this, the number of patches was further reduced to $p = 8$, which yielded no OOM exceptions. Finally, the dataset size was selected by employing the same trial and error approach, where a dataset size of $n = 945$ was yielded. The final sizes are a batch size of $b = 15$, number of patches $p = 10$ and dataset size $n = 945$. Specific details on how the dataset subset was generated can be found in section~\ref{subsec:data_preparation}. We can also calculate the number of epochs and the number of steps per epoch (update iterations). An epoch step is defined as model training for one batch. Each image only appears once across all batches, meaning our total number of steps per epoch is:
\[\frac{\textnormal{Dataset size}}{\textnormal{Batch size}} = \frac{945}{15} = 63\]
Following the calculation of the steps per epoch, we are able to calculate the number of epochs required for $10^5$ and $10^4$ update iterations:
\[\frac{10^5}{\textnormal{Steps per epoch}} = \frac{10^5}{63} \approx 1588\]
\[\frac{10^4}{\textnormal{Steps per epoch}} = \frac{10^5}{63} \approx 159\]
The selection of batch size, dataset size and number of patches allows for remaining training details to be considered. As suggested by Ledig et al.\ the Adam optimiser is used with $\beta_1 = 0.9$, the SRResNet model is trained with a learning rate of $10^{-4}$, the initial training for the SRGAN model is trained with a learning rate of $10^{-4}$ and, the fine-tuning training is executed with a learning rate of $10^{-5}$. The pretrained SRResNet model is used as the initial generator of the SRGAN model to reduce the likelihood of undesirable local optima~\cite{srgan}. The Ledig et al.\ procedure suggests $10^6$ update iterations for the pretrained SRResNet model. With the available training hardware and batch size, dataset size and number of patches training would take $\sim$48 hours. Due to the University of Leeds machines restarting every day, training the model for that long becomes tricky. As a result of this it makes sense to reduce the number of update iterations to $10^{5}$, reducing the training time to just over four hours. In interest of maintaining the guidance set out by Ledig et al.\ the number of update iterations for the first pass and fine-tune training of the SRGAN model should also be reduced by a factor of 10 to be proportional, yielding $10^4$ update iterations. Figure~\ref{table:model_training} provides a summary of the training parameters for all training scenarios.
\begin{table}
    \centering
    \begin{tabular}{cccc}
        \toprule
        {} & \textbf{SRResNet} & \textbf{SRGAN 1\textsuperscript{st} pass} & \textbf{SRGAN 2\textsuperscript{nd} pass} \\
        \midrule
        \textbf{Batch size} & 15 & 15 & 15\\ 
        \textbf{Patches} & 8 & 8 & 8 \\
        \textbf{Dataset size} & 945 & 945 & 945\\
        \textbf{Adam $\beta_1$} & 0.9 & 0.9 & 0.9\\
        \textbf{Learning rate} & $10^{-4}$ & $10^{-4}$ & $10^{-5}$ \\
        \textbf{Update iterations} & $10^5$ & $10^4$ & $10^4$ \\
        \textbf{Epochs} & 1588 & 159 & 159 \\
        \textbf{Steps per epoch} & 63 & 63 & 63 \\
        \bottomrule
    \end{tabular}
    \caption{Summary of the details and parameters used for model training.}
    \label{table:model_training}
\end{table}

At the end of each epoch, or one training cycle through the entire dataset, the model performance is validated using the validation set. As described in section~\ref{subsec:data_preparation}, 135 images, or three images for each of the 45 classes present in the NWPU-RESISC45 dataset compose the validation set. On epoch end, the validation images are processed (blurred and downsampled), and then fed through the SR reconstruction model. The losses are then calculated the same as they would be for each training step. A final validation loss is produced, which tells us how well the model performs on unseen remote sensing imagery as training proceeds. Once this loss is calculated at the end of every epoch, it is compared to the `best' or lowest validation loss that the model has produced in training. If the validation loss from the most recent epoch is lower than the previous best validation loss, then the current state of the model is saved, replacing the previous best state. This process repeats until model training is completed. The final saved model state will therefore be the state that produced the lowest loss when applied to unseen imagery, or the state of the model with the best SR reconstruction capabilities. The saved model states can then be loaded and applied to more imagery or trained further. They can be found within the project repository.


<Everything that comes under the `Methods' criterion in the mark scheme should be described in one, or possibly more than one, chapter(s).>

<Methods requirements. Is it clear that\dots
\begin{itemize}
    \item The solution exists?
    \item I have produced the deliverables specified?
    \item Appropriate steps or standards were taken to ensure a quality output?
    \item The challenges were clearly articulated?
\end{itemize}
>
