\chapter{Introduction and Background Research}

% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{chapter1}

% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\section{Introduction}

Super-resolution reconstruction describes the task of estimating a high-resolution representation of a low-resolution image~\cite{superResOverview}. It is a fundamental problem in the field of computer vision due to the ill-posed nature of the task: there are many `correct' HR representations of a single LR image~\cite{superResChallenges,superResRemoteSensingOverview}. SR image reconstruction is utilised in many domains, including remote sensing, medical imaging, surveillance, astronomy, and more~\cite{superResRemoteSensingChallenges, superResRemoteSensingOverview, superResMedicalImaging, superResSurveillance, superResAstronomy, superResUses}.

Remote sensing is the process of gathering data on an object without making physical contact~\cite{remoteSensing}. A predominant feature of remote sensing is aerial or satellite imagery, where images of the Earth are taken from aircraft of satellites respectively~\cite{ref}. Such images have proved useful in a variety of areas, including but not limited to urban planning~\cite{remoteSensingUses}, environmental analysis, land use management, and weather prediction~\cite{remoteSensingGANsReview}. \textcolor{blue}{References needed beyond this point.}

Remote sensing provides a particularly interesting set of conditions for the super resolution problem due to the constraints introduced by imaging hardware. Atmospheric conditions, hardware resolution and [something else here] can result in undesirable image quality, making SR reconstruction techniques attractive alternatives for producing high-resolution imagery.

SR reconstruction in remote sensing has matured as a field over recent years, with a host of proposed solutions to the problem. Traditional methods can be categorised as interpolation-based or reconstruction-based, with new learning-based methods proving particularly effective in the last decade. More recently the conception of generative deep learning architectures, such as generative adversarial networks, has led to a revived interest in the problem.

This project explores the effectiveness of GAN-based solutions to the super-resolution problem in remote sensing. A modification to a previous solution is suggested with the aim of improving model performance.

The problem: Super resolution in remote sensing. There exists a variety of solutions. Literature review on those solutions. Take SRGAN and improve it by improving the perceptual loss metric.

% Must provide evidence of a literature review. Use sections
% and subsections as they make sense for your project.
\clearpage
\section{Background research}\label{sec:background_research}
The following section details our investigation into the super-resolution reconstruction problem in remote sensing.

\subsection{The problem}\label{subsec:the_problem}
High-resolution (HR) images provide more detail than low-resolution (LR) images. The superior detail offered by HR imagery sanctions the existence of otherwise challenging exercises, including but not limited to weather prediction, urban mapping, and land cover observation~\cite{urbanMapping, mapping, cloudCover, vegetationMapping}. The most direct way of obtaining HR imagery is to increase the spatial resolution of the imaging hardware, however the amount of light available decreases and noise is introduced into the image~\cite{superResOverview}. An alternative method is to increase chip size to increase capacitance, but this results in undesireable inefficiencies and quickly becomes ineffective~\cite{superResOverview}. Increasing image resolution through hardware improvements is paired with high costs, and rapid changing application requirements do not complement the inflexible nature of such hardware~\cite{ref}. As a result, obtaining HR imagery becomes difficult when only improving imaging hardware.

The natural next step is to look to computational techniques to increase image resolution, leading us to the domain of super-resolution. The super-resolution problem describes the task of generating an HR representation from an LR image~\cite{ref}. We can formalise the problem as the inverse of the image degradation process, i.e.\ the process that turns some HR image into an LR image~\cite{imageDeg}. The image degradation process is as follows:
\begin{equation}\label{eq:image_deg}
    I_{LR} = D(B(I_{HR})) + \mathcal{N}
\end{equation}
This consists of three steps one-way functions: blurring ($B$), downsampling ($D$), and finally noise addition ($\mathcal{N}$). As each stage of the process is a one-way function the SR problem is ill-posed, i.e.\ multiple SR reconstructions could be perceived as `correct' for a single LR image~\cite{ref}.

Remote sensing describes the process of measuring the properties of objects without making physical contact, typically executed with aircraft or satellites~\cite{remoteSensing,remoteSensingImageProcessing}. As a species we rely on remote sensing for a variety of important practices, spanning numerous domains. This includes environmental assessment, global climate change detection, agriculture monitoring, renewable and non-renewable resource observation, meteorology and general weather analysis, land mapping, and military applications~\cite{remoteSensingImageProcessing, remoteSensingUses, remoteSensingGANsReview}.

Many spatial analyses require HR remote sensing imagery to be possible~\cite{ref}. We measure resolution of remote sensing imagery with spatial resolution, which is the physical area occupied by a single image pixel~\cite{ref}. In commerically available remote sensing datasets this ranges from 10cm to 1000m per pixel, with different applications for different resolutions~\cite{remoteSensingImageProcessing}. In many instances the resolution of imagery available for remote sensing application is too low, so it becomes very useful to construct HR imagery from LR inputs. Interestingly, we are able to describe equation~\ref{eq:image_deg} in real terms using remote sensing as our context. The blurring function, $B$, is introduced by the optical system capturing the spread of light, the downsampling function, $D$, describes the downsampling process enforced by the resolution of the sensing hardware, and $\mathcal{N}$ describes the addition of noise, usually Gaussian, imposed by atmospheric conditions or sensor failures~\cite{superResRemoteSensingOverview,superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}.

It is easy to see that SR has great potential in the field of remote sensing. This natural pairing, along with the importance of many of the practices conducted using remote sensing imagery, results in an interesting problem to solve with positive consequnces if successful. Therefore, investigating and developing solutions to the SR problem in remote sensing is the main goal of this project.

\subsection{Taxonomy of super-resolution solutions}
Before embarking on the investigation into various SR methods we must consider the taxonomy of the solution space. There are many different classifications we could consider based on various criteria. For example, we may choose to organise solutions based on the domain they operate in, resulting in a classification of either frequency domain or spatial domain~\cite{superResRemoteSensingOverview}. Alternatively, we could choose to split based on the knowledge of parameters we possess beforehand, leading to a classification of blind or non-blind~\cite{superResRemoteSensingOverview}. The complex nature of solutions, along with the increase of hybridised proposals, leads Fernandez-Beltran \etal\ to propose a simplified taxonomy of SR solutions. This taxonomy groups solutions based on their functional properties, and yields three categories: reconstruction-based, learning-based, and hybrid-based ~\cite{superResRemoteSensingOverview}. For this project we will consider each of these classes as described by Fernandez-Beltran \etal\ We also consider interpolation-based methods, primarily used for upsampling purposes, and some additional methods used specifically to produce HR remote sensing imagery.

\subsection{Interpolation-based solutions}
Interpolation-based solutions increase image resolution by upscaling then estimating the values of missing pixels. Upscaling an image by a factor of $n$ has the effect of increasing the number of pixels by $n^2$, introducing new pixels for which we have no reference value for in the LR image. Interpolation algorithms estimate the values of the unknown pixels by using the values of the known pixels and an assumed relationship between them. 

\subsubsection{Nearest neighbour interpolation}
Nearest neighbour interpolation uses the nearest neighbour algorithm to calculate the value of the missing pixels introduced by upscaling~\cite{nnInterpolation}. To find the value of an unknown pixel, we calculate the distance to the surrounding known pixels and then take the closest known pixel value for our unknown pixel. Using nearest neighbour interpolation means no new pixel values are introduced; we always take a pixel from the input image. This results in an upsampled image with sharp edges.

\subsubsection{Polynomial interpolation}
Polynomial interpolation methods assume a polynomial relationship between pixel values. Bilinear interpolation assumes a linear relationship, where unknown pixel values are estimated using a linearly weighted sum of the known surrounding pixel values. This is calculated using the known pixels from both axes, hence the bi in bilinear. Bicubic takes this process a step further, where a cubic relationship is assumed between pixel values. Instead of calculating a linearly weighted sum of immediate known pixel values we first calculate the curve between the known pixel values, known as a spline. To get the function of this spline we need to calculate the gradient at each of the known pixels, which itself is calculated using the pixel values surrounding those. Both of these methods introduce pixel values that did not exist in the input, where the change in pixel value is more gradual, creating a smoother image.

Polynomial interpolation methods are computationally simple, resulting in quick run times and use in real-time applications~\cite{interpolation}. The smoothing effect introduced by polynomial interpolation is sufficient for images with lots of low-frequency information as the gradual transition of pixel values does not produce much of an effect, but for images with lots of high-frequency information, such as edges and texture, this can produce an overly smooth result~\cite{interpolation}.

\subsubsection{Edge-directed interpolation}
Edge-directed interpolation has the primary aim of maintaining the sharpness of edges after the interpolation process~\cite{interpolation}. In polynomial interpolation we interpolate unknown pixel values based on proximity with no considerations for the contents of the image. In edge-directed interpolation we first attempt to understand the orientation of the edges in the image, and then interpolate along those edges. This stops the interpolation process skipping edges and results in their preservation. This difficulty here, however, is that estimation of the edge orientation is not necessarily straightforward from an image processing standpoint, as the edges within natural images are often blurred or noisy to some degree~\cite{interpolation}. As a result various methods for estimating edge orientations exist.

The fusion approach effectively `fuses' the estimations of several edges to provide a good overall estimation of the direction to interpolate along, and is aided by the fusion process being computationally inexpensive~\cite{interpolation}. New edge-directed interpolation (NEDI) takes this a step further by calculating local covariance matrices to determine how pixel values change with direction, where these directions are used to identify edge orientations. This dominant edge orientation is used to interpolate the values of new pixels producing a higher quality result when compared to the simpler fusion technique~\cite{ref}. Soft-decision interpolation determines edge directions similarly to NEDI, but then interpolates with various algorithms to produce several results. The quality of these results are assessed based on edge preservation and artefact avoidance, and a weighting is assigned correspondingly. This weighting is then applied to each of the interpolations to create a combined result that pulls features from various different interpolation algorithms~\cite{ref}.

\subsubsection{Critical analysis of interpolation-based solutions}
There are numerous variations of interpolation-based solutions to the SR problem, however they all share a fundamental flaw. Interpolation is only concerned with reversing the downsampling function in the image degradation process, $D$~\cite{interpolation}. They fail to account for blurring and noise addition which creates suboptimal results when applied to natural imagery that has undergone the entire degradation process. The main benefit of interpolation-based methods is they are generally computationally inexpensive, so for quick or real-time upsampling they may be used instead~\cite{interpolation}. The aim of this project is to produce SR reconstructions for remote sensing imagery, and remote sensing imagery has the image degradation process applied to it when captured (see section~\ref{subsec:the_problem}), so interpolation-based methods may not be sufficient when compared to the alternatives.

\subsection{Reconstruction-based solutions}
Reconstruction-based methods improve upon the interpolation methods by considering the blur and noise effects of image degradation, providing a more complete solution and therefore better results. The common theme within this category of solutions is a three step approach to the problem: interpolation, feature extraction, and reconstruction~\cite{superResRemoteSensingOverview}. Firstly, images are upsampled using a standard interpolation technique, then features are extracted, and finally the image is reconstructed by aggregating the upsampled image and the extracted features. This provides a general framework for reconstruction-based methods, but the exact process for each stage is dependent on the specific method~\cite{superResRemoteSensingOverview}.

\subsubsection{Iterative back projection}
Iterative back projection (IBP), as the name suggests, iteratively projects features of the LR image onto our SR reconstruction until some stopping criteria is met~\cite{ref}. The first step is to create an initial HR estimation, which can be random initialisation, zero-value intialisation, or the outcome from an interpolation algorithm. We then perform forward projection, where our HR estimation is downsampled, blurred, and noise is introduced to model the image degradation process. The error between the forward projected estimation and true LR image is calculated, and the error itself is upsampled. We then introduce the upsampled error to our HR estimation to produce an SR reconstruction that better resembles the LR image where noise is removed, is upsampled and deblurred. This process is repeated until some stopping criterion is met, for example a maximum number of iterations or a negligible decrease in error. The result is an SR reconstruction that has attempted to reverse all stages of the image degradation process. In practice IBP is simple to implement, however can be computationally expensive due to the number of iterations it takes to produce a desirable outcome~\cite{ref}.

\subsubsection{Projection onto convex sets}
TODO

\subsubsection{Point spread function deconvolution}
A point spread function (PSF) describes how light spreads when captured by imaging hardware, caused by diffraction, imperfections, or sensor failures, which has a blurring effect on the captured image~\cite{ref}. We can artificially produce a blurred image by marching a blurring kernel across it, which performs mathematical operations with the HR image pixels, a process known as convolution. If we know the blurring kernel, we are able reverse this process, or deconvolve, to obtain a reconstruction resembling the HR input. PSF deconvolution uses this idea and deconvolves the LR image with the inverse of some blur kernel that models the PSF that created the LR image. It is very likely that we do not know the exact blur kernel we must use, and it is mostly the case that there is no single blur kernel we can use to perfectly deconvolve an LR image, so the main difficulty becomes deciding the blur kernel we use to model the PSF. PSF deconvolution also suffers from noise amplification, where the blur kernel worsens noise in the LR image.

\subsubsection{Gradient profiling}
Gradient profiling uses specific kernels to calculate the gradients of pixel values, which can then be used guide us in creating an SR reconstruction~\cite{ref}. Firstly we create an HR estimation similar to IBP, where the most common method is to use an interpolation algorithm to upsample the LR image. We then calculate the gradients of both the LR image and our HR estimate through a convolution with a specific kernel. Some examples we can use are the Sobel operator, Prewitt operator, Laplacian operator, or the Canny edge detector. Once we have the gradients for both of the images, we use the gradients of the LR image to transform the gradients of the HR estimation to introduce the sharpness contained within the LR image. The transformed gradients of the HR estimation are then used to reconstruct our HR estimation into our final SR output. Similar to PSF deconvolution, gradient profiling is vulnerable to image noise, and also suffers from expensive computation introduced by the cost of calculating image gradients.

\subsubsection{Critical analysis of reconstruction-based solutions}

\subsection{Learning-based solutions}
Learning-based methods aim to learn a generalised relationship between LR and HR images instead of attempting to reconstruct some LR image directly. This relationship is learned from training data, or numerous paired examples of LR and HR images. Once this relationship is learned we are able to apply it to new imagery to generate an SR output. Some examples include\dots

\subsection{Hybrid solutions}

\subsection{Additional solutions}

\subsection{Project goals}

<Section requirements. Have I\dots
\begin{itemize}
    \item Explained the problem clearly?
    \item Identified relevant areas for investigation and discussed them in the report under appropriate headings and critically?
    \item Reviewed previous attempts to solve this and similar problems?
    \item Justified any claims made using credible primary or secondary sources?
\end{itemize}
>