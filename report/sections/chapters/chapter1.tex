\chapter{Introduction and Background Research}

% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{chapter1}

% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\section{Introduction}

Super-resolution (SR) reconstruction describes the task of estimating a high-resolution (HR) representation of a low-resolution (LR) image~\cite{superResOverview}. It is a fundamental problem in the field of computer vision due to the ill-posed nature of the task: there are many `correct' HR representations of a single LR image~\cite{superResChallenges,superResRemoteSensingOverview}. SR image reconstruction is utilised in many domains, including remote sensing, medical imaging, surveillance, astronomy, and more~\cite{superResRemoteSensingChallenges, superResRemoteSensingOverview, superResMedicalImaging, superResSurveillance, superResAstronomy, superResUses}.

Remote sensing is the process of gathering data on an object without making physical contact~\cite{remoteSensing}. A predominant feature of remote sensing is aerial or satellite imagery, where images of the Earth are taken from aircraft of satellites respectively~\cite{ref}. Such images have proved useful in a variety of areas, including but not limited to urban planning~\cite{remoteSensingUses}, environmental analysis, land use management, and weather prediction~\cite{remoteSensingGANsReview}. \textcolor{blue}{References needed beyond this point.}

Remote sensing provides a particularly interesting set of conditions for the super resolution problem due to the constraints introduced by imaging hardware. Atmospheric conditions, hardware resolution and [something else here] can result in undesirable image quality, making SR reconstruction techniques attractive alternatives for producing high-resolution imagery.

SR reconstruction in remote sensing has matured as a field over recent years, with a host of proposed solutions to the problem. Traditional methods can be categorised as interpolation-based or reconstruction-based, with new learning-based methods proving particularly effective in the last decade. More recently the conception of generative deep learning architectures, such as generative adversarial networks, has led to a revived interest in the problem.

This project explores the effectiveness of GAN-based solutions to the super-resolution problem in remote sensing. A modification to a previous solution is suggested with the aim of improving model performance.

The problem: Super resolution in remote sensing. There exists a variety of solutions. Literature review on those solutions. Take SRGAN and improve it by improving the perceptual loss metric.

% Must provide evidence of a literature review. Use sections
% and subsections as they make sense for your project.
\section{Background research}\label{sec:background_research}
The following section details our investigation into the super-resolution reconstruction problem in remote sensing.

\subsection{Super-resolution reconstruction}
HR imagery provides a greater level of detail than LR imagery, opening up a variety of applications such as training neural networks, urban mapping, predicting weather, tracking land cover, and more~\cite{hrTrainNN, urbanMapping, mapping, cloudCover, vegetationMapping}. One approach to increasing image resolution is to improve imaging hardware. Improving imaging hardware allows for consistent images of the specified resolution, but is costly and time-consuming. As a result, improving hardware requires strong incentives for those who fund such projects, reducing the likelihood for them to go ahead. Additionally, hardware is unable to traverse beyond its resolution limit, causing inflexibility in an environment where requirements are ever-changing~\cite{remoteSensingGANsReview}. 

The natural next step is to look to computational techniques for increasing the resolution of imagery. The result is the field of super-resolution reconstruction. The super-resolution image reconstruction problem describes the task of generating an HR representation of an LR image~\cite{ref}. Providing a solution to the problem requires solving the inverse of the image degradation process~\cite{imageDeg}.
\[I_{LR} = \mathcal{D}(\mathcal{B}(I_{HR})) + \mathcal{N}\]
The image degradation process consists of three steps, blurring, downsampling, and noise addition, where each step is a one-way function. This manufactures SR reconstruction as an ill-posed problem, i.e.\ numerous SR reconstructions could be perceived as `correct' for a singe LR image. Three classes of approach to the problem exist, which we cover with the remainder of this section.

\subsubsection{Interpolation-based methods}
Interpolation-based function by upscaling images and then using some algorithm to calculate the values of the missing pixels~\cite{interpolation}.

Nearest neighbour interpolation uses the nearest neighbour algorithm to calculate the value of the missing pixels introduced by upsampling~\cite{nnInterpolation}. To find the value of an unknown pixel, we calculate the distance to the surrounding known pixels, i.e.\ the pixels known from the input image, and then take the closest known pixel value to our unknown pixel. Using nearest neighbour interpolation means no new pixel values are introduced; we always take a pixel from the input image. This results in an upsampled image with sharp edges.

Bilinear interpolation takes this idea a step further and assumes a linear relationship between pixels~\cite{bilnearInterpolation}. We use this assumption to calculate the missing pixels, where the missing values between two pixels are filled by following the line mapped between the two known pixel values. For example, take two known pixels, $x_0$ and $x_4$ with values 100 and 200 respectively, in an image upsampled by a factor of four, i.e.\ three unknown values between $x_0$ and $x_4$, called $x_1, x_2, x_3$. Bilinear interpolation would result in $x_1, x_2, x_3$ having values 125, 150 and 175 respectively, that is, the values between the known pixels follow a linear relationship. We can extend this to two dimensions, where we can calculate the value of any pixel in an image.

Bicubic interpolation assumes a cubic relationship between two pixels~\cite{bicubicInterpolation}. It makes the explicit assumption that the unknown values between two pixels sit on a curve instead of a line. To calculate this curve, to then calculate the missing pixel values, we must look to the pixel values either side of our known pixels. We use these to first calculate the `gradient' of our known pixels which we can then use to calculate the curve between the two pixels, called a spline. This allows us to place the unknown pixel values on this line. This results in a much smoother transition between known pixels, but fails to capture edge features.

There are numerous adaptations of interpolation based methods, each with different applications and effectiveness, including but not limited to edge directed interpolation, soft decision interpolation and fusion methods~\cite{interpolation}. Interpolation methods are computationally effective and do not require much memory, however the fundamental flaw is they only provide a solution for the downsampling step of the image degradation process, and not anti-blur and denoising properties~\cite{interpolation}. As a result interpolation methods often fall-flat when applied to natural imagery that has undergone the entire image degradation process. 

\subsubsection{Reconstruction-based methods}

\subsubsection{Learning-based methods}

\subsection{Neural networks}\label{subsec:neural_networks}
Neural networks are collections of interconnected, artificial neurons designed to mimic the synaptic function of neurons in the brain~\cite{ref}. By mimicing the behaviour of neurons, neural networks aim to reproduce complex learning behaviours exhibited by living beings~\cite{ref}. As a result, we can use neural networks to learn mappings between inputs and outputs that are otherwise invisible or too complex to calculate~\cite{ref}.

The goal of a neural network is to learn an approximation of some unkown function that maps inputs to outptus~\cite{ref}. Each neural network is composed of a set of parameters, called weights and biases, which decide the configuration of the network and therefore the approximation of the unknown function~\cite{ref}. We measure how well the neural network has estimated the function through the use of a performance metric called loss~\cite{ref}. A higher loss value suggests worse approximation, and a lower loss value suggests better approximation.

Therefore, by minimsing this loss value we can improve the approximation of the unknown function~\cite{ref}. This minimisation is called training, and consists of exposing the neural network to numerous examples of inputs and the resulting outputs, so that it may learn what the mapping between the two is~\cite{ref}. At the end of each training iteration, the loss of the current configuration of parameters is calculated and use steepest gradient descent (see section~\ref{subsubsec:sgd}) to calculate how to nudge each parameter to produce a lower loss value. This process is repeated until we reach a point where we are satisfied with the value of our loss, and by extension the approximation of the unknown function.

Through this training process neural networks can learn to approximate any function, given that we have the appropriate inputs and outputs. Therefore, neural networks operate as universal function approximators~\cite{ref}. By changing what data we train a network with, along with the structure and arrangement of the neurons that compose the network architecture, we are able to achieve a massive range of goals~\cite{ref}.

\subsubsection{Gradient descent}\label{subsubsec:sgd}
Steepest gradient descent is an iterative optimisation algorithm used to find some minima of an $n$-variate function~\cite{ref}. It does this by calculating the partial derivtive of the loss function with respect to the parameters of the neural network.
\[\frac{\partial\mathcal{L}(\omega)}{\partial\omega}\]
We calculate this derivative using a process called backpropagation, where the gradients are calculated backwards through the neural network by successivley using the chain rule. This is necessary as the gradient of once layer in the network is conditional on the layer before and so forth. The value of the derivative is essentially the `gradient' of the loss function, or the value representing the rate that the loss function is increasing. So, by taking a small step in the negative direction of the gradient, we achieve the effect of reducing the loss function. To actually take this step `down' the gradient we must nudge each of the neural network parameters according to the calculation of the gradient. The entire process can be explained with the following formula:
\[\omega \leftarrow\omega - \eta\frac{\partial\mathcal{L}(\omega)}{\partial\omega}\]
$\omega$ represents the function parameters, $\eta$ is the size of the small step we take in the negative direction of the gradient, and $\frac{\partial\mathcal{L}(\omega)}{\partial\omega}$ is the gradient calculation itself. This process is repeated iteratively during training time until some stopping criterion is met.

\subsubsection{Optimisation}
As we increase the size of a neural network, the number of trainable parameters increasing exponentially, requiring optimisation techniques if we wish to implement complex architectures~\cite{ref}. A common optimisation method for training neural networks is stochastic gradient descent, where instead of training the neural network using the entire dataset and then calculating the gradient, we calculate the gradient one training example at a time~\cite{ref}. This is more computationally efficient and uses less memory as we are performing fewer calculations concurrently. The downside is that training becomes less stable, as a single training example is not representative of the whole dataset. A popular middle ground is batch gradient descent, where a batch of training examples are used to calculate the gradient~\cite{ref}. This has the advantage of being faster than vanilla gradient descent, whilst also being more stable than stochastic gradient descent. Another optimisation technique used is the adaptive moment optimiser, also known as Adam~\cite{ref}. Adam adaptively changes the learning rate, $\eta$, to promote more stable and efficient training. It effectively avoids using a learning rate too large or too small depending on how close the loss function is to a local minima.

\subsection{Convolutional neural networks}
Convolutional neural networks (CNN) rearrange the structure of neurons into a grid, allowing us to represent and learn from images. This simple architecture change presents neural networks as the solution to complicated image processing problems.

The main function of a CNN is the convolution process, where an $n \times n$ patch, called a kernel, sequentially `marches' across the image input and applies mathematical operations. Traditionally, this kernel has been used for a variety of purposes, including blurring\dots, where each of the processes uses a different kernel to result in the desired effect. By changing the kernel, we are able to achieve different results. Setting up our weights and biases as the values of these kernels allows us to learn the kernels that produce the desired output when convolved over an image. We can then apply more learned kernels to the previous output to create a more complex representation of the image. By repeating these convolutions we are able to create very complex representations of the image inputs, which we can then use to perform image processing tasks such as classification, generation, denoising, etc.

\subsection{Generative adversarial networks}
Generative adverarial networks (GAN) were first propsed by Goodfellow \etal \ in 2014, and are designed to generate imagery. They consist of two elements, a generator and a discriminator, where training resembles a zero-sum game~\cite{ref} between the two. The ultiamte goal is to produce a trained generator that generates imagery resembles the training dataset.

This is achieved by adversarial training, where the generator and discriminator are trained alternately. The discriminator is trained to distinguish the difference between real imagery (training data) and fake imagery (output from the generator). The generator learns how to fool the discriminator by generating images that accurately resemble the training data. Both components iteratively learn from the other. Following a full training process, the generator is saved to be reapplied to generate new instances of images that resemble the training data.

\subsubsection{Generator}
The traditional generator acrhitecture constis of an input of random noise, followed by several transposing convolutions to turn the noise into imagery based on the learned kernels, with a final convolution to turn the upsampled noise into an image format of $W \times H \times C$, where $W$ is the image width, $H$ is the image height, and $C$ is the number of channels (3 for an RGB image). The activation of each neuron in this final layer represents the pixel value, i.e.\ brightness of that colour channel for that pixel.

\subsubsection{Discriminator}
The discriminator acrhitecture is implemented as a binary classifer, where the goal is to classify the image as either real or fake. This is usually implemented as several downsampling convolutional layers, where the aim of the learned kernels is to extract the most important image features each class, i.e.\ the image features that make a real image look real, and the image features that make a fake image look fake. Following the downsampling convolutional layers are a series of fully-connected layers (see section~\ref{subsec:neural_networks}) which reduce the number of neurons down to one, where the final neuron activation represents the class of the input image.

\subsubsection{Adversarial loss}
To learn from one another the components play a zero-sum adversarial game, where a success for one component means a loss for another, and vice versa. The aim is to iteratively improve both models so that they may continuously learn more and more complex features at the same rate. This is enacted with the adversarial loss component:
\[\min_G\max_D\mathbb{E}_x[\log D(x)] + \mathbb{E}_z[\log(1 - D(G(z)))]\]
$D(x)$ is the probability that the discriminator classifies $x$ as real. $x$ represents the real images, so if the discriminator is acheiving its goal the probability will be high. We then take the logarithm of this value to produce a large value the closer we get to a probability of 1, i.e.\ perfect classification of real images by the discriminator. $G(z)$ represents the images generated when noise, $z$, is passed to the generator component. We then pass this to the discriminator component and take the log. A successful discriminator will wish to produce a low probability of classifying $G(z)$ as real, so we subtract the result from 1 to ensure that the discriminator is still aiming to maximise this portion of the equation. The reverse is true for the generator, which is aiming to minimise this formula to produce images that consistently fool the discriminator.

Goodfellow \etal \ provide an optimisation to the adversarial loss function. In the early stages of training when the generator produces poor results, it becomes easy for the discriminator to quickly identify fake images and results in a weak gradient for the generator to learn from~\cite{gan}. To fix this, we can reframe the minimisation of $\log(1 - D(G(z)))$ to the maximisation of $\log D(G(z))$. This essentially changes the problem from `minimise the number of fake classifications' to `maximise the number of real classifications'. Implementing this change provides a stronger gradient for the generator and consequently produces better training results.

\subsection{Super-resolution generative adversarial network}
The SRGAN model, proposed by Ledig et al.\ in 2017, introduced a new approach to the super-resolution problem~\cite{srgan}. By employing a generative adversarial network architecture researchers were able to achieve state-of-the-art results, consequently kickstarting the large-scale development of GAN-based super-resolution solutions. Since then, numerous adaptations of the SRGAN model have been proposed with many surpassing the SR reconstruction capabilities of SRGAN~\cite{models}. Regardless, SRGAN remains the most important and influential GAN-based SR reconstruction model.

\subsubsection{Architecture}
The SRGAN model has two components, the generator and the discriminator. The generator model is responsible for upscaling LR imagery to produce the SR output, and the discriminator is responsible for providing a goal for the generator during adversarial training. The generator of the SRGAN, named SRResNet, was also proposed by Ledig et al. SRResNet employs a residual block structure to increase the resolution of the input imagery, where a series of residual blocks with skip connections learn the features of the dataset. The residual blocks are followed by two upsample operations executed with the pixel eshuffle technique. During training, the model learns the best features to extract from the training dataset in order to then upsample. New imagery can then be passed through SRResNet where the learned filters are applied, the upsampling operation is executed, and SR imagery is produced. SRResNet generates sufficient SR imagery independent of the discriminator component of SRGAN, however the results often fail to capture high-frequency information such as texture and detailed objects~\cite{srgan}. The discriminator is introduced to further improve the SRResNet model, where adversarial training and the perceptual loss metric aid in generating imagery where the high-frequency components are not lost, and the final output is as close as possible to the original HR image. The discriminator component follows a stereotypical CNN classifier architecture, where the image features are captured and reduced, and finally flattened into a fully connected sequence that reduces to a single neuron. The output of the neuron is passed through a sigmoid function and the output represents the classification of real or fake for the generated images.

\subsubsection{Perceptual loss}

\subsection{Remote sensing}
Remote sensing describes the process of measuring the properties of objects without making physical contact, typically executed with aircraft or satellites~\cite{remoteSensing,remoteSensingImageProcessing}. As a species we rely on remote sensing for a variety of important practices, spanning numerous domains. This includes, but is not limited to, environmental assessment, global climate change detection, agriculture monitoring, renewable and non-renewable resource observation, meteorology and general weather analysis, land mapping, and military applications~\cite{remoteSensingImageProcessing, remoteSensingUses, remoteSensingGANsReview}.

We may split remote sensing data into two categories: image-centered and data-centered. Image centered remote sensing data primarily focuses on the spatial characteristics of the object being measured, hence the representation as an image~\cite{remoteSensingImageProcessing}. The data-centered category describes the analysis of data captured using different spectral bands, for example to measure forest density, atmospheric coniditions, and ocean properties~\cite{remoteSensingImageProcessing}.

Performing spatial analysis using the image-centered category of remote sensing data requires a certain level of clarity to be possible. We measure such clarity using spatial resolution, which describes the physical area represented by a single image pixel~\cite{ref}. Spatial resolutions vary massively based on sensing hardware and application requirements, yielding resolutions between 10cm and 1000m per pixel~\cite{remoteSensing,remoteSensingImageProcessing}. Measuring the melting rate of glaciers requires a large spatial resolution, whereas performing military surveillance on a dwelling would require a spatial resolution from the lower end of the spectrum.

Many applications of remote sensing require HR imagery to be possible. HR imagery is composed of a large number of pixels, meaning more pixels are available to represent objects and a greater level of detail is achieved. Unfortunately, building remote sensing hardware capable of capturing HR imagery becomes difficult due to physical constraints and exceptionally high costs~\cite{ref}. 

The capturing of remote sesing imagery is often described using a three step process, where the information recieved by sensors is blurred, then downsampled, with a final addition of noise~\cite{superResRemoteSensingOverview, superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}. The process can be expressed with the following equation:
\[I_{LR} = D(B(I_{HR})) + \mathcal{N}\]
$B$ describes the process of blurring, introduced by the optical system capturing the spread of light, $D$ describes the downsampling process enforced by the resolution of the sensing hardware, and $\mathcal{N}$ describes the addition of noise, usually Gaussian, imposed by atmospheric conditions or sensor failures. As a result, many in the field have turned to computational post processing methods, such as super-resolution reconstruction, to remediate the degradation processed introduced by sensing hardware~\cite{superResRemoteSensingOverview}. 

\subsection{Remote sensing-specific super-resolution methods}
<Section requirements. Have I\dots
\begin{itemize}
    \item Explained the problem clearly?
    \item Identified relevant areas for investigation and discussed them in the report under appropriate headings and critically?
    \item Reviewed previous attempts to solve this and similar problems?
    \item Justified any claims made using credible primary or secondary sources?
\end{itemize}
>