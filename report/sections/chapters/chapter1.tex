\chapter{Introduction and Background Research}

% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{chapter1}

% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\section{Introduction}

Super-resolution reconstruction describes the task of estimating a high-resolution representation of a low-resolution image~\cite{superResOverview}. It is a fundamental problem in the field of computer vision due to the ill-posed nature of the task: there are many `correct' HR representations of a single LR image~\cite{superResChallenges,superResRemoteSensingOverview}. SR image reconstruction is utilised in many domains, including remote sensing, medical imaging, surveillance, astronomy, and more~\cite{superResRemoteSensingChallenges, superResRemoteSensingOverview, superResMedicalImaging, superResSurveillance, superResAstronomy, superResUses}.

Remote sensing is the process of gathering data on an object without making physical contact~\cite{remoteSensing}. A predominant feature of remote sensing is aerial or satellite imagery, where images of the Earth are taken from aircraft of satellites respectively~\cite{ref}. Such images have proved useful in a variety of areas, including but not limited to urban planning~\cite{remoteSensingUses}, environmental analysis, land use management, and weather prediction~\cite{remoteSensingGANsReview}. \textcolor{blue}{References needed beyond this point.}

Remote sensing provides a particularly interesting set of conditions for the super resolution problem due to the constraints introduced by imaging hardware. Atmospheric conditions, hardware resolution and [something else here] can result in undesirable image quality, making SR reconstruction techniques attractive alternatives for producing high-resolution imagery.

SR reconstruction in remote sensing has matured as a field over recent years, with a host of proposed solutions to the problem. Traditional methods can be categorised as interpolation-based or reconstruction-based, with new learning-based methods proving particularly effective in the last decade. More recently the conception of generative deep learning architectures, such as generative adversarial networks, has led to a revived interest in the problem.

This project explores the effectiveness of GAN-based solutions to the super-resolution problem in remote sensing. A modification to a previous solution is suggested with the aim of improving model performance.

The problem: Super resolution in remote sensing. There exists a variety of solutions. Literature review on those solutions. Take SRGAN and improve it by improving the perceptual loss metric.

% Must provide evidence of a literature review. Use sections
% and subsections as they make sense for your project.
\clearpage
\section{Background research}\label{sec:background_research}
The following section details our investigation into the super-resolution reconstruction problem in remote sensing.
\subsection{The problem}
High-resolution (HR) images provide more detail than low-resolution (LR) images. The superior detail offered by HR imagery sanctions the existence of otherwise challenging exercises, including but not limited to weather prediction, urban mapping, and land cover observation~\cite{urbanMapping, mapping, cloudCover, vegetationMapping}. The most direct way of obtaining HR imagery is to increase the spatial resolution of the imaging hardware, however the amount of light available decreases and noise is introduced into the image~\cite{superResOverview}. An alternative method is to increase chip size to increase capacitance, but this results in undesireable inefficiencies and quickly becomes ineffective~\cite{superResOverview}. Increasing image resolution through hardware improvements is paired with high costs, and rapid changing application requirements do not complement the inflexible nature of such hardware~\cite{ref}. As a result, obtaining HR imagery becomes difficult when only improving imaging hardware.

The natural next step is to look to computational techniques to increase image resolution, leading us to the domain of super-resolution. The super-resolution problem describes the task of generating an HR representation from an LR image~\cite{ref}. We can formalise the problem as the inverse of the image degradation process, i.e.\ the process that turns some HR image into an LR image~\cite{imageDeg}. The image degradation process is as follows:
\begin{equation}\label{eq:image_deg}
    I_{LR} = D(B(I_{HR})) + \mathcal{N}
\end{equation}
This consists of three steps one-way functions: blurring ($B$), downsampling ($D$), and finally noise addition ($\mathcal{N}$). As each stage of the process is a one-way function the SR problem is ill-posed, i.e.\ multiple SR reconstructions could be perceived as `correct' for a single LR image~\cite{ref}.

Remote sensing describes the process of measuring the properties of objects without making physical contact, typically executed with aircraft or satellites~\cite{remoteSensing,remoteSensingImageProcessing}. As a species we rely on remote sensing for a variety of important practices, spanning numerous domains. This includes environmental assessment, global climate change detection, agriculture monitoring, renewable and non-renewable resource observation, meteorology and general weather analysis, land mapping, and military applications~\cite{remoteSensingImageProcessing, remoteSensingUses, remoteSensingGANsReview}.

Many spatial analyses require HR remote sensing imagery to be possible~\cite{ref}. We measure resolution of remote sensing imagery with spatial resolution, which is the physical area occupied by a single image pixel~\cite{ref}. In commerically available remote sensing datasets this ranges from 10cm to 1000m per pixel, with different applications for different resolutions~\cite{remoteSensingImageProcessing}. In many instances the resolution of imagery available for remote sensing application is too low, so it becomes very useful to construct HR imagery from LR inputs. Interestingly, we are able to describe equation~\ref{eq:image_deg} in real terms using remote sensing as our context. The blurring function, $B$, is introduced by the optical system capturing the spread of light, the downsampling function, $D$, describes the downsampling process enforced by the resolution of the sensing hardware, and $\mathcal{N}$ describes the addition of noise, usually Gaussian, imposed by atmospheric conditions or sensor failures~\cite{superResRemoteSensingOverview,superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}.

It is easy to see that SR has great potential in the field of remote sensing. This natural pairing, along with the importance of many of the practices conducted using remote sensing imagery, results in an interesting problem to solve with positive consequnces if successful. Therefore, investigating and developing solutions to the SR problem in remote sensing is the main goal of this project.

\subsection{Super-resolution taxonomy}

\subsection{Existing solutions}
\subsubsection{Interpolation-based}
\subsubsection{Reconstruction-based}
\subsubsection{Learning-based}
\subsubsection{Additional solutions}
\subsection{Project goals}

\subsubsection{Interpolation-based methods}
Interpolation-based function by upscaling images and then using some algorithm to calculate the values of the missing pixels~\cite{interpolation}.

Nearest neighbour interpolation uses the nearest neighbour algorithm to calculate the value of the missing pixels introduced by upsampling~\cite{nnInterpolation}. To find the value of an unknown pixel, we calculate the distance to the surrounding known pixels, i.e.\ the pixels known from the input image, and then take the closest known pixel value to our unknown pixel. Using nearest neighbour interpolation means no new pixel values are introduced; we always take a pixel from the input image. This results in an upsampled image with sharp edges.

Bilinear interpolation takes this idea a step further and assumes a linear relationship between pixels~\cite{bilnearInterpolation}. We use this assumption to calculate the missing pixels, where the missing values between two pixels are filled by following the line mapped between the two known pixel values. For example, take two known pixels, $x_0$ and $x_4$ with values 100 and 200 respectively, in an image upsampled by a factor of four, i.e.\ three unknown values between $x_0$ and $x_4$, called $x_1, x_2, x_3$. Bilinear interpolation would result in $x_1, x_2, x_3$ having values 125, 150 and 175 respectively, that is, the values between the known pixels follow a linear relationship. We can extend this to two dimensions, where we can calculate the value of any pixel in an image.

Bicubic interpolation assumes a cubic relationship between two pixels~\cite{bicubicInterpolation}. It makes the explicit assumption that the unknown values between two pixels sit on a curve instead of a line. To calculate this curve, to then calculate the missing pixel values, we must look to the pixel values either side of our known pixels. We use these to first calculate the `gradient' of our known pixels which we can then use to calculate the curve between the two pixels, called a spline. This allows us to place the unknown pixel values on this line. This results in a much smoother transition between known pixels, but fails to capture edge features.

There are numerous adaptations of interpolation based methods, each with different applications and effectiveness, including but not limited to edge directed interpolation, soft decision interpolation and fusion methods~\cite{interpolation}. Interpolation methods are computationally effective and do not require much memory, however the fundamental flaw is they only provide a solution for the downsampling step of the image degradation process, and not anti-blur and denoising properties~\cite{interpolation}. As a result interpolation methods often fall-flat when applied to natural imagery that has undergone the entire image degradation process. 

\subsubsection{Reconstruction-based methods}
Reconstruction-based methods improve upon the interpolation methods by considering the blur and noise effects of image degradation, providing a more complete solution and therefore better results. The common theme within this category of solutions is a three step approach to the problem: interpolation, feature extraction, and reconstruction~\cite{superResRemoteSensingOverview}. Firstly, images are upsampled using a standard interpolation technique, then features are extracted, and finally the image is reconstructed by aggregating the upsampled image and the extracted features. This provides a general framework for reconstruction-based methods, but the exact process for each stage is dependent on the specific method~\cite{superResRemoteSensingOverview}. Some notable examples include iterative back projection, point spread function deconvolution and gradient profiles.~\cite{superResRemoteSensingOverview}. Reconsutrction-based methods are important to mention in this project as the first wave of true SR techniques, however we will not cover them in detail due to the superiority of newer, learning-based methods.

\subsubsection{Learning-based methods}
Learning-based methods aim to learn a generalised relationship between LR and HR images instead of attempting to reconstruct some LR image directly. This relationship is learned using training data, or numerous paired examples of LR and HR images. Once this relationship is learned we are able to apply it to new imagery to generate an SR output. Some examples include\dots

\subsection{Neural networks}\label{subsec:neural_networks}
Neural networks are collections of interconnected, artificial neurons designed to mimic the synaptic function of neurons in the brain~\cite{ref}. By mimicing the behaviour of neurons, neural networks aim to reproduce complex learning behaviours exhibited by living beings~\cite{ref}. As a result, we can use neural networks to learn mappings between inputs and outputs that are otherwise invisible or too complex to calculate~\cite{ref}.

The goal of a neural network is to learn an approximation of some unkown function that maps inputs to outptus~\cite{ref}. Each neural network is composed of a set of parameters, called weights and biases, which decide the configuration of the network and therefore the approximation of the unknown function~\cite{ref}. We measure how well the neural network has estimated the function through the use of a performance metric called loss~\cite{ref}. A higher loss value suggests worse approximation, and a lower loss value suggests better approximation.

Therefore, by minimsing this loss value we can improve the approximation of the unknown function~\cite{ref}. This minimisation is called training, and consists of exposing the neural network to numerous examples of inputs and the resulting outputs, so that it may learn what the mapping between the two is~\cite{ref}. At the end of each training iteration, the loss of the current configuration of parameters is calculated and use steepest gradient descent (see section~\ref{subsubsec:sgd}) to calculate how to nudge each parameter to produce a lower loss value. This process is repeated until we reach a point where we are satisfied with the value of our loss, and by extension the approximation of the unknown function.

Through this training process neural networks can learn to approximate any function, given that we have the appropriate inputs and outputs. Therefore, neural networks operate as universal function approximators~\cite{ref}. By changing what data we train a network with, along with the structure and arrangement of the neurons that compose the network architecture, we are able to achieve a massive range of goals~\cite{ref}.

\subsubsection{Gradient descent}\label{subsubsec:sgd}
Steepest gradient descent is an iterative optimisation algorithm used to find some minima of an $n$-variate function~\cite{ref}. It does this by calculating the partial derivtive of the loss function with respect to the parameters of the neural network.
\[\frac{\partial\mathcal{L}(\omega)}{\partial\omega}\]
We calculate this derivative using a process called backpropagation, where the gradients are calculated backwards through the neural network by successivley using the chain rule. This is necessary as the gradient of once layer in the network is conditional on the layer before and so forth. The value of the derivative is essentially the `gradient' of the loss function, or the value representing the rate that the loss function is increasing. So, by taking a small step in the negative direction of the gradient, we achieve the effect of reducing the loss function. To actually take this step `down' the gradient we must nudge each of the neural network parameters according to the calculation of the gradient. The entire process can be explained with the following formula:
\[\omega \leftarrow\omega - \eta\frac{\partial\mathcal{L}(\omega)}{\partial\omega}\]
$\omega$ represents the function parameters, $\eta$ is the size of the small step we take in the negative direction of the gradient, and $\frac{\partial\mathcal{L}(\omega)}{\partial\omega}$ is the gradient calculation itself. This process is repeated iteratively during training time until some stopping criterion is met.

\subsubsection{Loss functions}

\subsubsection{Optimisation}
As we increase the size of a neural network, the number of trainable parameters increasing exponentially, requiring optimisation techniques if we wish to implement complex architectures~\cite{ref}. A common optimisation method for training neural networks is stochastic gradient descent, where instead of training the neural network using the entire dataset and then calculating the gradient, we calculate the gradient one training example at a time~\cite{ref}. This is more computationally efficient and uses less memory as we are performing fewer calculations concurrently. The downside is that training becomes less stable, as a single training example is not representative of the whole dataset. A popular middle ground is batch gradient descent, where a batch of training examples are used to calculate the gradient~\cite{ref}. This has the advantage of being faster than vanilla gradient descent, whilst also being more stable than stochastic gradient descent. Another optimisation technique used is the adaptive moment optimiser, also known as Adam~\cite{ref}. Adam adaptively changes the learning rate, $\eta$, to promote more stable and efficient training. It effectively avoids using a learning rate too large or too small depending on how close the loss function is to a local minima.

\subsection{Convolutional neural networks}
Convolutional neural networks (CNN) rearrange the structure of neurons into a grid, allowing us to represent and learn from images. This simple architecture change presents neural networks as the solution to complicated image processing problems.

The main function of a CNN is the convolution process, where an $n \times n$ patch, called a kernel, sequentially `marches' across the image input and applies mathematical operations. Traditionally, this kernel has been used for a variety of purposes, including blurring\dots, where each of the processes uses a different kernel to result in the desired effect. By changing the kernel, we are able to achieve different results. Setting up our weights and biases as the values of these kernels allows us to learn the kernels that produce the desired output when convolved over an image. We can then apply more learned kernels to the previous output to create a more complex representation of the image. By repeating these convolutions we are able to create very complex representations of the image inputs, which we can then use to perform image processing tasks such as classification, generation, denoising, etc.

\subsection{Generative adversarial networks}
Generative adverarial networks (GAN) were first propsed by Goodfellow \etal \ in 2014, and are designed to generate imagery. They consist of two elements, a generator and a discriminator, where training resembles a zero-sum game~\cite{ref} between the two. The ultiamte goal is to produce a trained generator that generates imagery resembles the training dataset.

This is achieved by adversarial training, where the generator and discriminator are trained alternately. The discriminator is trained to distinguish the difference between real imagery (training data) and fake imagery (output from the generator). The generator learns how to fool the discriminator by generating images that accurately resemble the training data. Both components iteratively learn from the other. Following a full training process, the generator is saved to be reapplied to generate new instances of images that resemble the training data.

\subsubsection{Generator}
The traditional generator acrhitecture constis of an input of random noise, followed by several transposing convolutions to turn the noise into imagery based on the learned kernels, with a final convolution to turn the upsampled noise into an image format of $W \times H \times C$, where $W$ is the image width, $H$ is the image height, and $C$ is the number of channels (3 for an RGB image). The activation of each neuron in this final layer represents the pixel value, i.e.\ brightness of that colour channel for that pixel.

\subsubsection{Discriminator}
The discriminator acrhitecture is implemented as a binary classifer, where the goal is to classify the image as either real or fake. This is usually implemented as several downsampling convolutional layers, where the aim of the learned kernels is to extract the most important image features each class, i.e.\ the image features that make a real image look real, and the image features that make a fake image look fake. Following the downsampling convolutional layers are a series of fully-connected layers (see section~\ref{subsec:neural_networks}) which reduce the number of neurons down to one, where the final neuron activation represents the class of the input image.

\subsubsection{Adversarial loss}
To learn from one another the components play a zero-sum adversarial game, where a success for one component means a loss for another, and vice versa. The aim is to iteratively improve both models so that they may continuously learn more and more complex features at the same rate. This is enacted with the adversarial loss component:
\[\min_G\max_D\mathbb{E}_x[\log D(x)] + \mathbb{E}_z[\log(1 - D(G(z)))]\]
$D(x)$ is the probability that the discriminator classifies $x$ as real. $x$ represents the real images, so if the discriminator is acheiving its goal the probability will be high. We then take the logarithm of this value to produce a large value the closer we get to a probability of 1, i.e.\ perfect classification of real images by the discriminator. $G(z)$ represents the images generated when noise, $z$, is passed to the generator component. We then pass this to the discriminator component and take the log. A successful discriminator will wish to produce a low probability of classifying $G(z)$ as real, so we subtract the result from 1 to ensure that the discriminator is still aiming to maximise this portion of the equation. The reverse is true for the generator, which is aiming to minimise this formula to produce images that consistently fool the discriminator.

Goodfellow \etal \ provide an optimisation to the adversarial loss function. In the early stages of training when the generator produces poor results, it becomes easy for the discriminator to quickly identify fake images and results in a weak gradient for the generator to learn from~\cite{gan}. To fix this, we can reframe the minimisation of $\log(1 - D(G(z)))$ to the maximisation of $\log D(G(z))$. This essentially changes the problem from `minimise the number of fake classifications' to `maximise the number of real classifications'. Implementing this change provides a stronger gradient for the generator and consequently produces better training results.

\subsection{Super-resolution generative adversarial network}
The SRGAN model, proposed by Ledig et al.\ in 2017, introduced a new approach to the super-resolution problem~\cite{srgan}. By employing a generative adversarial network architecture researchers were able to achieve state-of-the-art results, consequently kickstarting the large-scale development of GAN-based super-resolution solutions. Since then, numerous adaptations of the SRGAN model have been proposed with many surpassing the SR reconstruction capabilities of SRGAN~\cite{models}. Regardless, SRGAN remains the most important and influential GAN-based SR reconstruction model.

\subsubsection{Architecture}
SRGAN follows the traditional GAN architecture, with both a generator and discriminator. The discriminator component follows a stereotypical image classifier architecture, where the image features are extracted and reduced by a sequence of convolutional blocks, proceeded by a fully connected sequence that reduces to a single neuron. The output of the neuron is passed through a sigmoid function and the output represents the classification of real or fake for the generated images.

The generator component of SRGAN is an SR reconstruction solution in its own right. SRResNet consists of a series of residual blocks that successivley capture image features, followed by an upsample section that increases the resolution of the input image. We can train SRResNet using mean squared error (MSE). During training time, the pixel-wise difference between the training instance and the SR reconstruction is calculated using MSE providing a value to minimise to create SR reconstructions that better resmeble the training instance. SRResNet is able to create sufficient SR reconstructions, however using MSE loss often fails to capture high-frequency image information such as texture~\cite{srgan}. This happens as the MSE loss only minimises pixel-wise differences, and does not take the importance of local regions into account. To remediate this, a pretrained SRResNet model is used as the generator for SRGAN, where the adversarial training process introdcues a greater consideration for image features lost through using MSE.\@ It executes this improved learning with aid from a perceptual loss component introduced to the adversarial loss.

\subsubsection{Perceptual loss}
The perceptual loss component of the SRGAN loss function is designed to measure the `perceputal' differences between HR training instances and SR reconstructions~\cite{srgan}. Instead of focusing on the pixel-to-pixel differences, perceptual loss measures the differences of features extracted from the image. By comparing image features we can find the difference between local regions of interest in the image, which provides context whilst learning and generates better results~\cite{ref}. Perceptual loss for SRGAN is implemented using the VGG19 image classifier~\cite{srgan}. VGG19, released by Visual Geometry Group, is a 19 layer image classification network. The network extracts image features and then downsamples to result in a series of feature maps. We can think of these feature maps as high level representations of the image. By calculating the Euclidian distance between the HR and SR feature maps we produce a scalar value representing the perceptual difference. Combining this value with the adversarial loss component yields a loss function that aims to produce outputs that are perceptually similar to the input image, whilst also fooling our trained disciminator. This two-fold apporach produces superior SR reconstructions when compared to the vanilla SRResNet-MSE.\@ Ledig \etal. formalise the perceptual loss component as:
\[\mathcal{L} = \mathcal{L}_{VGG} + 10^{-3} \mathcal{L}_{Gen}\]
where $\mathcal{L}_{VGG}$ is the loss captured using the VGG feature maps and $\mathcal{L}_{Gen}$ is the adversarial loss. The adversarial loss component is scaled by a factor of $10^{-3}$.

\subsection{Performance metrics}
\subsubsection{Structural similarity index measure}
The structural similarity index measure (SSIM) was introduced in 2004 by Wang \etal\ with the aim of providing a image quality measure that focuses on the degradation of structure from a refernce image~\cite{ssim}. It achieves this goal by comparing three properties of the two imgaes: luminance, contrast, and structure. Image luminance is defined as the average pixel value across all image channels, constrast is the standard deviation of pixel values, and structure is calcualted by mean-centering the input image and dividing by the standard deviation to give a normalised distribution. Each of these values is compared across both images, and the resulting output is a float representing the structural similarity. SSIM is a better tool for comparison of images then traditional error-based solutions such as pixel-wise MSE, however poses a significant increase in computational compared to traditional methods and as a result benefits from optimisation~\cite{ssim}.

\subsubsection{Peak signal-to-noise ratio}
Peak signal-to-noise ration (PSNR) is used to compare some processed signal to its source signal. It provides a measure of how much a signal has changed after a degradation process, calculated as the ratio between the maximum possible power a signal can have and the power of the distorting noise introduced into the image~\cite{psnr}. The range between the maximum power and distortion power can be large, so the logarithm of the ratio is taken and the resulting value is measured in decibles (dB). As PSNR measures the effect of a degradation process, it is a useful metric for SR reconstruction performance measurement as it effectively provides a value representing how well we have reversed the image degradation process. PSNR is widely adopted as a valid performance metric in image processing, however may not provide the same quality of comparson as SSIM, due to SSIM being modelled on human perception of image differences.~\cite{psnrAnalysis}.

\subsubsection{Mean opinion score}
Mean opinion score (MOS) is used as a performance metric across numerous domains. MOS relies on human judgement of outputs, where judgements are given as a 1 to 5 Likert scale score. The average of all scores provided by all participants is taken to provide the final MOS for an output. MOS is very simple to understand and compute, however has the fundamental limitation of being entirely opinion-based and is not deterministic. MOS requires numerous human participants to review images, and as a result suffers from the standard issues encountered when operating with human participants such as\dots

\subsection{Remote sensing-specific super-resolution methods}
<Section requirements. Have I\dots
\begin{itemize}
    \item Explained the problem clearly?
    \item Identified relevant areas for investigation and discussed them in the report under appropriate headings and critically?
    \item Reviewed previous attempts to solve this and similar problems?
    \item Justified any claims made using credible primary or secondary sources?
\end{itemize}
>