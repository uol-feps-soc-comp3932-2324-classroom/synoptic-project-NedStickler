\chapter{Introduction and Background Research}

% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{chapter1}

% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\section{Introduction}

Super-resolution reconstruction describes the task of estimating a high-resolution representation of a low-resolution image~\cite{superResOverview}. It is a fundamental problem in the field of computer vision due to the ill-posed nature of the task: there are many `correct' HR representations of a single LR image~\cite{superResChallenges,superResRemoteSensingOverview}. SR image reconstruction is utilised in many domains, including remote sensing, medical imaging, surveillance, astronomy, and more~\cite{superResRemoteSensingChallenges, superResRemoteSensingOverview, superResMedicalImaging, superResSurveillance, superResAstronomy, superResUses}.

Remote sensing is the process of gathering data on an object without making physical contact~\cite{remoteSensing}. A predominant feature of remote sensing is aerial or satellite imagery, where images of the Earth are taken from aircraft of satellites respectively~\cite{ref}. Such images have proved useful in a variety of areas, including but not limited to urban planning~\cite{remoteSensingUses}, environmental analysis, land use management, and weather prediction~\cite{remoteSensingGANsReview}. \textcolor{blue}{References needed beyond this point.}

Remote sensing provides a particularly interesting set of conditions for the super resolution problem due to the constraints introduced by imaging hardware. Atmospheric conditions, hardware resolution and [something else here] can result in undesirable image quality, making SR reconstruction techniques attractive alternatives for producing high-resolution imagery.

SR reconstruction in remote sensing has matured as a field over recent years, with a host of proposed solutions to the problem. Traditional methods can be categorised as interpolation-based or reconstruction-based, with new learning-based methods proving particularly effective in the last decade. More recently the conception of generative deep learning architectures, such as generative adversarial networks, has led to a revived interest in the problem.

This project explores the effectiveness of GAN-based solutions to the super-resolution problem in remote sensing. A modification to a previous solution is suggested with the aim of improving model performance.

The problem: Super resolution in remote sensing. There exists a variety of solutions. Literature review on those solutions. Take SRGAN and improve it by improving the perceptual loss metric.

% Must provide evidence of a literature review. Use sections
% and subsections as they make sense for your project.
\clearpage
\section{Background research}\label{sec:background_research}
The following section details our investigation into the super-resolution reconstruction problem in remote sensing.

\subsection{The problem}\label{subsec:the_problem}
High-resolution (HR) images provide more detail than low-resolution (LR) images. The superior detail offered by HR imagery sanctions the existence of otherwise challenging exercises, including but not limited to weather prediction, urban mapping, and land cover observation~\cite{urbanMapping, mapping, cloudCover, vegetationMapping}. The most direct way of obtaining HR imagery is to increase the spatial resolution of the imaging hardware, however the amount of light available decreases and noise is introduced into the image~\cite{superResOverview}.\ \textcolor{blue}{An alternative method is to increase chip size to increase capacitance, but this results in undesirable inefficiencies and quickly becomes ineffective~\cite{superResOverview}}. Increasing image resolution through hardware improvements is paired with high costs, and rapidly changing application requirements do not complement the inflexible nature of such hardware~\cite{ref}. As a result, obtaining HR imagery becomes difficult when only improving imaging hardware.

The natural next step is to look to computational techniques to increase image resolution, leading us to the domain of super-resolution. The super-resolution problem describes the task of generating an HR representation of an LR image~\cite{ref}. We can formalise the problem as the inverse of the image degradation process, i.e.\ the process that turns some HR image into an LR image~\cite{imageDeg}. The image degradation process is as follows:
\begin{equation}\label{eq:image_deg}
    I_{LR} = D(B(I_{HR})) + \mathcal{N}
\end{equation}
This consists of three one-way functions: blurring ($B$), downsampling ($D$), and finally noise addition ($\mathcal{N}$). As each stage of the process is a one-way function the SR problem is ill-posed, i.e.\ multiple SR reconstructions could be perceived as `correct' for a single LR image~\cite{ref}.

Remote sensing describes the process of measuring the properties of objects without making physical contact, typically executed with aircraft or satellites~\cite{remoteSensing,remoteSensingImageProcessing}. As a species we rely on remote sensing for a variety of important practices, spanning numerous domains. This includes environmental assessment, global climate change detection, agriculture monitoring, renewable and non-renewable resource observation, meteorology and general weather analysis, land mapping, and military applications~\cite{remoteSensingImageProcessing, remoteSensingUses, remoteSensingGANsReview}.

Many spatial analyses require HR remote sensing imagery to be possible~\cite{ref}. We measure resolution of remote sensing imagery with spatial resolution, which is the physical area occupied by a single image pixel~\cite{ref}. In commercially available remote sensing datasets this ranges from 10cm to 1000m per pixel, with different applications for different resolutions~\cite{remoteSensingImageProcessing}. In many instances the resolution of imagery available for remote sensing application is too low, so it becomes very useful to construct HR imagery from LR inputs. Interestingly, we are able to describe equation~\ref{eq:image_deg} in real terms using remote sensing as our context. The blurring function, $B$, is introduced by the optical system capturing the spread of light, the downsampling function, $D$, describes the downsampling process enforced by the resolution of the sensing hardware, and $\mathcal{N}$ describes the addition of noise, usually Gaussian, imposed by atmospheric conditions or sensor failures~\cite{superResRemoteSensingOverview,superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}.

It is easy to see that SR has great potential in the field of remote sensing. This natural pairing, along with the importance of many of the practices conducted using remote sensing imagery, results in an interesting problem to solve with positive consequences if successful. Therefore, investigating and developing solutions to the SR problem in remote sensing is the main goal of this project.

\subsection{Taxonomy of super-resolution solutions}
Before embarking on the investigation into various SR methods we must consider the taxonomy of the solution space. There are numerous classifications we could consider based on various criteria. For example, we may choose to organise solutions based on the domain they operate in, resulting in a classification of either frequency domain or spatial domain~\cite{superResRemoteSensingOverview}. Alternatively, we could choose to split based on the knowledge of parameters we possess beforehand, leading to a classification of blind or non-blind~\cite{superResRemoteSensingOverview}. The complex nature of solutions, along with the increase of hybridised proposals, leads Fernandez-Beltran \etal\ to propose a simplified taxonomy of SR solutions. This taxonomy groups solutions based on their functional properties, and yields three categories: reconstruction-based, learning-based, and hybrid-based~\cite{superResRemoteSensingOverview}. Further investigation showed that another taxonomy to consider is the interpolation, reconstruction, and learning-based taxonomy instead, where we group hybrid methods into either reconstruction or learning based~\cite{remoteSensingDeepLearningReview, remoteSensingGANsReview, isrgan, tesagan}. Given that much of the recent literature uses the interpolation-, reconstruction-, and learning-based taxonomy, we will adopt the same for this project. We will also investigate pansharpening, a method used to increase the spatial resolution of remote sensing imagery.

\subsection{Interpolation-based solutions}
Interpolation-based solutions increase image resolution by upscaling then estimating the values of missing pixels. Upscaling an image by a factor of $n$ has the effect of increasing the number of pixels by $n^2$, introducing new pixels for which we have no reference value for in the LR image. Interpolation algorithms estimate the values of the unknown pixels by using the values of the known pixels and an assumed relationship between them. 

\subsubsection{Nearest neighbour interpolation}
Nearest neighbour interpolation uses the nearest neighbour algorithm to calculate the value of the missing pixels introduced by upscaling~\cite{nnInterpolation}. To find the value of an unknown pixel, we calculate the distance to the surrounding known pixels and then take the closest known pixel value for our unknown pixel. Using nearest neighbour interpolation means no new pixel values are introduced; we always take a pixel from the input image. This results in an upsampled image with sharp edges.

\subsubsection{Polynomial interpolation}
Polynomial interpolation methods assume a polynomial relationship between pixel values. Bilinear interpolation assumes a linear relationship, where unknown pixel values are estimated using a linearly weighted sum of the known surrounding pixel values. This is calculated using the known pixels from both axes, hence the bi in bilinear. Bicubic takes this process a step further, where a cubic relationship is assumed between pixel values. Instead of calculating a linearly weighted sum of immediate known pixel values we first calculate the curve between the known pixel values, known as a spline. To get the function of this spline we need to calculate the gradient at each of the known pixels, which itself is calculated using the pixel values surrounding those. Both of these methods introduce pixel values that did not exist in the input, where the change in pixel value is more gradual, creating a smoother image.

Polynomial interpolation methods are computationally simple, resulting in quick run times and use in real-time applications~\cite{interpolation}. The smoothing effect introduced by polynomial interpolation is sufficient for images with lots of low-frequency information as the gradual transition of pixel values does not produce much of an effect, but for images with lots of high-frequency information, such as edges and texture, this can produce an overly smooth result~\cite{interpolation}.

\subsubsection{Edge-directed interpolation}
Edge-directed interpolation has the primary aim of maintaining the sharpness of edges after the interpolation process~\cite{interpolation}. In polynomial interpolation we interpolate unknown pixel values based on proximity with no considerations for the contents of the image. In edge-directed interpolation we first attempt to understand the orientation of the edges in the image, and then interpolate along those edges. This stops the interpolation process skipping edges and results in their preservation. This difficulty here, however, is that estimation of the edge orientation is not necessarily straightforward from an image processing standpoint, as the edges within natural images are often blurred or noisy to some degree~\cite{interpolation}. As a result various methods for estimating edge orientations exist.

The fusion approach effectively `fuses' the estimations of several edges to provide a good overall estimation of the direction to interpolate along, and is aided by the fusion process being computationally inexpensive~\cite{interpolation}. New edge-directed interpolation (NEDI) takes this a step further by calculating local covariance matrices to determine how pixel values change with direction, where these directions are used to identify edge orientations. This dominant edge orientation is used to interpolate the values of new pixels producing a higher quality result when compared to the simpler fusion technique~\cite{ref}. Soft-decision interpolation determines edge directions similarly to NEDI, but then interpolates with various algorithms to produce several results. The quality of these results are assessed based on edge preservation and artefact avoidance, and a weighting is assigned correspondingly. This weighting is then applied to each of the interpolations to create a combined result that pulls features from various different interpolation algorithms~\cite{ref}.

\subsubsection{Critical analysis of interpolation-based solutions}
There are numerous variations of interpolation-based solutions to the SR problem, however they all share a fundamental flaw. Interpolation is only concerned with reversing the downsampling function in the image degradation process, $D$~\cite{interpolation}. They fail to account for blurring and noise addition which creates suboptimal results when applied to natural imagery that has undergone the entire degradation process. The main benefit of interpolation-based methods is they are generally computationally inexpensive, so for quick or real-time upsampling they may be used instead~\cite{interpolation}. The aim of this project is to produce SR reconstructions for remote sensing imagery, and remote sensing imagery has the image degradation process applied to it when captured (see section~\ref{subsec:the_problem}), so interpolation-based methods may not be sufficient when compared to the alternatives.

\subsection{Reconstruction-based solutions}
Reconstruction-based methods improve upon the interpolation methods by considering the blur and noise effects of image degradation, providing a more complete solution and therefore better results. The common theme within this category of solutions is a three step approach to the problem: interpolation, feature extraction, and reconstruction~\cite{superResRemoteSensingOverview}. Firstly, images are upsampled using a standard interpolation technique, then features are extracted, and finally the image is reconstructed by aggregating the upsampled image and the extracted features. This provides a general framework for reconstruction-based methods, but the exact process for each stage is dependent on the specific method~\cite{superResRemoteSensingOverview}.

\subsubsection{Iterative back projection}
Iterative back projection (IBP), as the name suggests, iteratively projects features of the LR image onto our SR reconstruction until some stopping criteria is met~\cite{ref}. The first step is to create an initial HR estimation, which can be random initialisation, zero-value intialisation, or the outcome from an interpolation algorithm. We then perform forward projection, where our HR estimation is downsampled, blurred, and noise is introduced to model the image degradation process. The error between the forward projected estimation and true LR image is calculated, and the error itself is upsampled. We then introduce the upsampled error to our HR estimation to produce an SR reconstruction that better resembles the LR image where noise is removed, is upsampled and deblurred. This process is repeated until some stopping criterion is met, for example a maximum number of iterations or a negligible decrease in error. The result is an SR reconstruction that has attempted to reverse all stages of the image degradation process. In practice IBP is simple to implement, however can be computationally expensive due to the number of iterations it takes to produce a desirable outcome~\cite{ref}.

\subsubsection{Projection onto convex sets}
Projection onto convex set (POCS) is a generalised mathematical tool used to find solutions that fit a set of predefined constraints. We may utilise POCS for the SR reconstruction task; if we set our constraints in a specific way we can find an acceptable solution. The first step is to define the appropriate constraints for our reconstruction, this might include pixel values, shapes and features we wish to conserve within the image, the required smoothness or sharpness. We represent these constraints as a high-dimentional convex set, where the mathematical definition of a convex set is `for any two points in a convex set, the line joining them remains entirely within the set'. This property allows the projection process to happen, i.e. taking our LR image and projecting it to a higher resolution and within our defined constraints. The next step is to guess an initial image, which similar to the previous reconstruction-based methods may be randomly initialised, zero-valued, or the output of some interpolation method. Next, the image inital image estimation is projected onto the constraining convex sets, where the closest point to the projection in the set is determined. This is repeated for all constraining sets, where applying each constraint aligns the image estimation with the HR ground truth image. The entire process is iterated until convergence, i.e. negligible change across projections. Whilst POCS can handle a variety of constraints to produce good SR results, it may not always converge, and more crucically, if a constraint cannot be represented as a convex set then it cannot be enforced on the output image.

\subsubsection{Point spread function deconvolution}
A point spread function (PSF) describes how light spreads when captured by imaging hardware, caused by diffraction, imperfections, or sensor failures, which has a blurring effect on the captured image~\cite{ref}. We can artificially produce a blurred image by marching a blurring kernel across it, which performs mathematical operations with the HR image pixels, a process known as convolution. If we know the blurring kernel, we are able reverse this process, or deconvolve, to obtain a reconstruction resembling the HR input. PSF deconvolution uses this idea and deconvolves the LR image with the inverse of some blur kernel that models the PSF that created the LR image. It is very likely that we do not know the exact blur kernel we must use, and it is mostly the case that there is no single blur kernel we can use to perfectly deconvolve an LR image, so the main difficulty becomes deciding the blur kernel we use to model the PSF.\@ PSF deconvolution also suffers from noise amplification, where the blur kernel worsens noise in the LR image.

\subsubsection{Gradient profiling}
Gradient profiling uses specific kernels to calculate the gradients of pixel values, which can then be used guide us in creating an SR reconstruction~\cite{ref}. Firstly we create an HR estimation similar to IBP, where the most common method is to use an interpolation algorithm to upsample the LR image. We then calculate the gradients of both the LR image and our HR estimate through a convolution with a specific kernel. Some examples we can use are the Sobel operator, Prewitt operator, Laplacian operator, or the Canny edge detector. Once we have the gradients for both of the images, we use the gradients of the LR image to transform the gradients of the HR estimation to introduce the sharpness contained within the LR image. The transformed gradients of the HR estimation are then used to reconstruct our HR estimation into our final SR output. Similar to PSF deconvolution, gradient profiling is vulnerable to image noise, and also suffers from expensive computation introduced by the cost of calculating image gradients.

\subsubsection{Critical analysis of reconstruction-based solutions}
The primary success of reconstruction-based solutions is the significant improvement over traditional interpolation-based methods. Reconstruction-based methods account for each stage of the image degradation process so produce less blur and noise. A significant downside of reconstruct-based solutions is their relative computational complexity. Many of the operations are complex optimisation problems, and difficulties converging can result in massive execution durations. Reconstruction-based problems also lose out due to the initial interpolation step, which remove detail from the image~\cite{remoteSensingGANsReview}.

\subsection{Learning-based solutions}
Learning-based solutions aim to learn the function that maps LR imagery to HR imagery. Once the mapping is learned it can be applied to unseen imagery to create an SR reconstruction. The method used to learn the LR to HR mapping varies across approaches.

\subsubsection{Sparse representation}
Sparse representation utilises the sparsity principle to create SR reconstructions, where the sparsity principle states that a signal (in our case an image) can be sparsely represented using some combination of basis functions. For image reconstruction, a dictionary of basis functions is learned from the HR images, where each of the basis function represents some feature of the image, for example, objects, texture, edges, etc. Learning from the HR images is conducted specifically to produce a small number of basis fucntions to ensure sparsity. Following the dictionary learning process we take the sparse representation of LR imagery in the terms of the learned dictionary. This then allows us to upscale the LR image and apply to learned basis functions to introduce the desired features that we mentoined earlier. The final result is an upsampled image that captures the key components of the LR image. Representing the reversal of the iamge degradation process as a sequence of basis functions allows us to create complex reconstructions, however actually performing the process is often expensive due to the difficult optimisation problems we need to solve to find sparse representations of images.

\subsubsection{Neighbourhood embedding}
Neighbourhood embedding uses learned knowledge of image patches to create SR reconstructions. The first step in the process is to extract patches from the LR image, which are then analysed to retrieve information about what is contained within. This information could be texture, edges, intensity, gradients etc. These patches are then embedded into a higher-dimensional space to discover complex relationshipships between them which are otherwise invisible. Next the mapping from LR patch to HR patch is learned through training. We can use this mapping to identify which HR patches to use to represent the LR image, which we then stitch together to produce the SR reconstruction. Neighbourhood embedding allows us to preserve detail through the complex relationships learned between patches when embedded in a higher dimensional space. The main downside of neighbourhood is that the process of doing so is incredibly computationally intensive.

\subsubsection{Mapping}
The universal goal of learning-based mapping approaches is to learn the function that maps some LR image to a corresponding HR image, which can then be applied to unseen imagery to produce a SR reconstruction. Whilst mapping methods are linked by this common goal, the way that they achieve it can be drastically different~\cite{superResRemoteSensingOverview}. Fernandez-Beltran \etal\ outline some of the key mapping approaches, such as nearest neighbour mapping, kernel ridge regression, and other regression regularisation approaches. Nearest neighbour mapping reconstructs images using a mapping between LR and HR patches using the nearest neighbour algorithm to identify the most appropriate HR patch to assign, whilst kernel ridge regression performs complex non-linear regression using ridge regularisation to avoid overfitting. Also beloning to this set of solutions are deep learning-based approaches. Deep learning uses neural network architectures to approximate the function that defines the relationship between the input data and output data, which is the key idea behind the mapping approach. The significant level deep learning-related SR reconstruction research suggests that our attention should be focused on the deep learning domain if we are to consider a mapping approach, therefore we will cover deep learning approaches in the following standalone section.

\subsubsection{Critical analysis of learning-based solutions}
Learning-based solutions offer significantly more flexible solutions when compared to the large number of assumptions made to execute reconstruction-based solutions. Learning-based solutions produce results that are more perceptually similar to the HR ground truth images. A large risk with learning-based solutions is the overfitting problem, where the solution fits well to the data it is trained on but has poor generalisation capabilities. Alongside this, large amounts of good-quality data is required to produce good results with learning-based solutions~\cite{superResRemoteSensingOverview}.

\subsection{Deep learning-based solutions}
Neural networks are the building blocks of deep learning. Known as universal function approximators, neural networks are able to learn the mapping between any input and output given sufficient training data~\cite{ref}. Convolutional neural networks offer a structural change to neural networks that allows us to represent the networks as images, and as a result learn complex hierarchical mappings of local regions of an image~\cite{ref}. Similar to other mapping approaches, training a neural network on pairs of LR and HR images allows us to learn some generalised function between the two. We can then apply this mapping to unseen LR images to produce an SR reconstruction. This logic is the foundation of deep learning based approaches to the SR reconstruction problem. Deep learning-based approaches have proved so efficient at creating SR reconstructions that the solution space is vast and with many different styles of approach. Here we overview the key deep learning-based solutions to the problem.

\subsubsection{Deep learning background}
Before investigating various deep learning-based solutions, it is useful to overview the components and practices within the field. Deep learning utilises neural networks to learn complex details from a dataset. A series of connected neurons are iteratively trained with inputs and their corresponding outputs to learn a generalised mapping between the two. The network guesses what the mapping should be, and we repeatedly measure how wrong it is using a scalar value called loss. We can frame neural network training as an optimisation problem, where we tweak the network parameters in such a way so that the loss value is minimised, effectively meaning our estimation of the mapping is as good as possible. This behaviour makes neural networks universal function approximators, allowing us to learn incredibly complex mappings between inputs and outputs.

By arranging our neurons in a grid like structure we can represent image data. Pairing this with a convolution of a kernel allows us to learn from local regions within images, opening up a variety of applications. This type of neural network is known as a convolutional neural network (CNN), and lets us learn hierarchical image features, which we can then use to classify images, create lower-dimensional representations, or even create SR reconstructions.

By taking inspiration from game theory, we can model our training process after a zero-sum game. Generative adversarial networks utilise this framework, where two components, a generator and discriminator, play the zero-sum game to iteratively improve their performance. The aim of the generator is to create imagery resembling the training dataset, and the aim of the discriminator is to distinguish between real images from the training data and fake images created by the generator. Both components learn from eachother, allowing them to iteratively improve. The final goal is to produce a generator that is so good at creating imagery resembling the training data that it fools the discriminator. This lets us use the generator to `make up' examples of the training set.

\subsubsection{SRCNN}
Super-resolution Convolutional Neural Network (SRCNN) was proposed in 2015 by Dong \etal\ as a CNN-based solution to the SR reconstruction problem~\cite{srcnn}. The solution architecture constist of three convolutions: patch extraction into a high-dimensional representation, non-linear mapping into HR, and finally image reconstruction. The first convolution is designed to extract appropraite features from the LR image, which are then transformed into a high-dimentional representation, which itself is then mapped into the HR space, which maps the LR feature maps into HR.\@ The image is then reconstructed from the HR feature maps to give the SR reconstruction. At the time of release, SRCNN outperformed state-of-the-art solutions, however showed very slow convergence times and slow training~\cite{srcnn}.

\subsubsection{VDSR}
After SRCNN laid the groundwork for CNN-based SR reconstruction solutions, Very Deep Super Resolution (VDSR) was proposed by Kim \etal\ and introduced a significantly deeper solution architecutre~\cite{vdsr}. VDSR employs successive feature extracting layers that retrieve the most important features, alongwith a skip connection between the output of the feature extraction section and the input image. The output of the addition operation between the two is the SR reconstruction. VDSR addressed the slow convergence time of SRCNN through the use of a skip connection, as well as producing significantly better results than its predecessor.

\subsubsection{SRGAN}
In 2017, Ledig \etal\ proposed Super-resolution Generative Adversarial Network (SRGAN) as a solution to the SR reconstruction problem~\cite{srgan}. SRGAN was the first GAN-based SR reconstruction solution, utilising the adversarial training process alongside a new loss function to help improve upon results. The generator component of SRGAN, named SRResNet, is itself a SR solution, with the adversarial training process added in to provide further clarity to the image. SRGAN again showed state-of-the-art results when compared to other reconstruction-based methods, and provided the community with the groundwork for more GAN-based solutions.

\subsubsection{ESRGAN}
Wang \etal\ build on SRGAN by modifying the architecture and loss component to bring the SR reconstructions closer to HR ground truth images. Enhanced Super-resolution Generative Adversarial Network (ESRGAN) implements changes threefold~\cite{esrgan}. Firstly, the residual block structure of SRGAN is altered to have a higher capacity whilst also being easier to train. Next, the discriminator component is changed to work relativistically, where images are compared instead of being labelled real or fake. Finally, the perceptual loss component is changed so that activations are taken before the VGG19 activations and not after. The result is are SR reconstructions that maintain the sharpness of images and beat the outputs from SRGAN.\@

\subsubsection{Other deep learning-based solutions}
The community remains active, with new solutions being proposed regularly. Here we touch on some additional deep learning-based SR reconstruction solutions that are worth mentioning. Improved SRGAN (ISRGAN) was suggested by Xiong \etal\ with the aim of addressing the unstable training and overly smooth results of SRGAN.\@ ISRGAN modifies the loss function to support more stable training and alters the network architecture to  avoid afforementioned smooth results, resulting in better generalisation capabilities~\cite{isrgan,remoteSensingGANsReview}. Texture Enhancement Self Attention Generative Adversarial Network (TE-SAGAN) utilises a self-attention module to ensure texture retention within the SR reconstruction~\cite{tesagan, remoteSensingGANsReview}. Novel Deep Super-resolution Generative Adversarial Network (NDSRGAN) uses numerous dense blocks to avoid distortions in the outputs~\cite{ndsrgan, remoteSensingGANsReview}. 

\subsubsection{Critical analysis of deep learning-based solutions}
We can see from the large number of proposed solutions that deep learning-based approaches are effective at solving the SR reconstruction problem. Deep learning-based solutions offer brilliant generalisation capabilities, are more efficient than traditional methods, and are the chosen development direction of the community~\cite{ndsrgan}. Whilst very attractive options, some solutions suffer from extensive complexity, and the solution space itself is highly saturated, making it difficult to offer state-of-the-art results with incremental model improvements. Deep learning-based solutions, similarly to traditional learning solutions, require a sufficient training set to produce good results, which is not always available~\cite{superResRemoteSensingOverview}.

\subsection{Pansharpening}
Pansharpening is a remote sensing-specific technique for increasing the resolution of imagery. Imaging hardware captures the panchromatic light spectrum as well as visible light. The visible light image is in colour but is LR because of the smaller spectrum. The panchromatic image is greyscale but is HR because over the wider spectrum that extends beyond the visible range. Pansharpening involves fusing the panchromatic image with the visible light imagery. The result is a higher-resolution image, but at the cost of some spectral resolution, caused by the removal of colour detail.

\subsubsection{Critical analysis of additional solutions}
Pansharpening is a simple technique to create HR remote sensing imagery, making it very cost effective, but would not class as a SR reconstruction technique. The process does not aim to reverse the entire image degradation process, so is less effective than reconstruction or learning-based methods. Another crucial flaw is that the panchromatic image is vulnerable to the same issues that the visible light image, such as noise and blur. As soon as the panchromatic image is affected we are not longer able to perform effective pansharpening.

\section{Project goals}
Following the review of the previous solutions to the SR reconstruction problem we can set out our project aim and goals. The aim of this project is to provide a solution to the super-resolution reconstruction problem in remote sensing. To achieve this aim, we set out the following project goals based on the findings from this chapter:
\begin{enumerate}
    \item Select an appropriate deep learning-based SR reconstruction method to implement.
    \item Investigate remote sensing datasets for training, validation, and testing of the deep-learning based SR reconstruction method.
    \item Adapt a feature of the deep learning-based method to improve SR reconstruction capabilities.
    \item Train the deep learning-based method using the selected remote sensing dataset.
    \item Evaluate the SR reconstruction capabilities of the adapted method trained on the remote sensing dataset.
    \item Produce better SR reconstructions than at least bicubic interpolation.
\end{enumerate}
\textcolor{blue}{Not much on this beforehand.}

<Section requirements. Have I\dots
\begin{itemize}
    \item Explained the problem clearly?
    \item Identified relevant areas for investigation and discussed them in the report under appropriate headings and critically?
    \item Reviewed previous attempts to solve this and similar problems?
    \item Justified any claims made using credible primary or secondary sources?
\end{itemize}
>