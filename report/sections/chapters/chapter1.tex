\chapter{Introduction and Background Research}

% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{chapter1}

% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\section{Introduction}

Super-resolution reconstruction describes the task of estimating a high-resolution representation of a low-resolution image~\cite{superResOverview}. It is a fundamental problem in the field of computer vision due to the ill-posed nature of the task: there are many `correct' high-resolution representations of a single low-resolution image~\cite{superResChallenges,superResRemoteSensingOverview}. Super-resolution reconstruction is utilised in many domains, including remote sensing, medical imaging, surveillance, astronomy, and more~\cite{superResRemoteSensingChallenges, superResRemoteSensingOverview, superResMedicalImaging, superResSurveillance, superResAstronomy, superResUses}.

Remote sensing is the process of gathering data on an object without making physical contact~\cite{remoteSensing}. A predominant class of remote sensing data is aerial or satellite imagery, where images of the Earth are taken from aircraft or satellites~\cite{ref}. Such images have proved useful in a variety of areas, including but not limited to urban planning~\cite{remoteSensingUses}, environmental analysis, land use management, and weather prediction~\cite{remoteSensingGANsReview}.

Remote sensing provides a particularly interesting set of conditions for the super resolution problem due to the constraints introduced by imaging hardware. Optical systems, hardware resolution and atmospheric conditions can result in undesirable image quality, making super-resolution reconstruction techniques attractive alternatives for producing high-resolution imagery.

Super-resolution reconstruction in remote sensing has matured as a field over recent years, with a host of proposed solutions to the problem. Existing methods can be categorised as interpolation-based, reconstruction-based, or learning-based methods, with the deep learning subset of learning methods proving particularly effective. More recently, the conception of generative deep learning architectures, such as generative adversarial networks, has led to a revived interest in the problem.

This project explores the super-resolution reconstruction problem in remote sensing in depth, covering the vast solution space via the community-accepted taxonomy of solutions. The main aim of the project is to provide a solution to the super-resolution problem in remote sensing having considered exisiting solutions, hardware constrains, and project scope.

% Must provide evidence of a literature review. Use sections
% and subsections as they make sense for your project.
\clearpage
\section{Background research}\label{sec:background_research}
The following section details our investigation into the super-resolution reconstruction problem in remote sensing.

\subsection{The problem}\label{subsec:the_problem}
High-resolution (HR) images provide more detail than low-resolution (LR) images. The superior detail offered by HR imagery sanctions the existence of otherwise challenging exercises, including weather prediction, urban mapping, and land cover observation~\cite{urbanMapping, mapping, cloudCover, vegetationMapping}. The most direct way of obtaining HR imagery is to increase the spatial resolution of the imaging hardware, however the amount of light available decreases and noise is introduced into the image~\cite{superResOverview}. An alternative method is to expand chip size to increase capacitance, but this results in undesirable physical inefficiencies and quickly becomes ineffective~\cite{superResOverview}. Increasing image resolution through hardware improvements is paired with high costs, and rapidly changing application requirements do not complement the inflexible nature of such hardware~\cite{ref}. As a result, obtaining HR imagery solely through hardware means becomes difficult.

The natural next step is to look to computational techniques to increase image resolution, leading us to the domain of SR. The SR problem describes the task of generating an HR representation of an LR image~\cite{ref}. We can formalise the problem as the inverse of the image degradation process, i.e.\ the process that turns some HR image into an LR image~\cite{imageDeg}. The image degradation process is as follows:
\begin{equation}\label{eq:image_deg}
    I_{LR} = D(B(I_{HR})) + \mathcal{N}
\end{equation}
This consists of three one-way functions: blurring ($B$), downsampling ($D$), and finally noise addition ($\mathcal{N}$). As each stage of the process is a one-way function the SR problem is ill-posed, i.e.\ multiple SR reconstructions could be perceived as `correct' for a single LR image~\cite{ref}.

Remote sensing describes the process of measuring the properties of objects without making physical contact, typically executed with aircraft or satellites~\cite{remoteSensing,remoteSensingImageProcessing}. As a species we rely on remote sensing for a variety of important practices, spanning numerous domains. This includes environmental assessment, global climate change detection, agriculture monitoring, renewable and non-renewable resource observation, meteorology and general weather analysis, land mapping, and military applications~\cite{remoteSensingImageProcessing, remoteSensingUses, remoteSensingGANsReview}.

Many spatial analyses require HR remote sensing imagery to be possible~\cite{ref}. We measure resolution of remote sensing imagery with spatial resolution, which is the physical area occupied by a single image pixel~\cite{ref}. In commercially available remote sensing datasets this ranges from 10 centimetres to 1000 metres per pixel, with different applications for different resolutions~\cite{remoteSensingImageProcessing}. In many instances the resolution of imagery available for remote sensing application is too low, so it becomes very useful to construct HR imagery from LR inputs. Interestingly, we are able to describe equation~\ref{eq:image_deg} in real terms using remote sensing as our context. The blurring function, $B$, is introduced by the optical system capturing the spread of light, the downsampling function, $D$, describes the downsampling process enforced by the resolution of the sensing hardware, and $\mathcal{N}$ describes the addition of noise, usually Gaussian, imposed by atmospheric conditions or sensor failures~\cite{superResRemoteSensingOverview,superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}.

It is easy to see that SR has great potential in the field of remote sensing. This natural pairing, along with the importance of many of the practices conducted using remote sensing imagery, results in an interesting problem to solve with positive consequences if successful. Therefore, investigating and developing solutions to the SR problem in remote sensing is the main goal of this project.

\subsection{Taxonomy of super-resolution solutions}
Before embarking on the investigation into various SR methods we must consider the taxonomy of the solution space. There are numerous classifications we could consider based on various criteria. For example, we may choose to organise solutions based on the domain they operate in, resulting in a classification of either frequency domain or spatial domain~\cite{superResRemoteSensingOverview}. Alternatively, we could choose to split based on the knowledge of parameters we possess beforehand, leading to a classification of blind or non-blind~\cite{superResRemoteSensingOverview}. The complex nature of solutions, along with the increase of hybridised proposals, leads Fernandez-Beltran \etal\ to propose a simplified taxonomy of SR solutions. This taxonomy groups solutions based on their functional properties, and yields three categories: reconstruction-based, learning-based, and hybrid-based~\cite{superResRemoteSensingOverview}. Alternatively, another taxonomy to consider removes the hybrid category and suggests interpolation-based, reconstruction-based, and learning-based taxonomy instead~\cite{remoteSensingDeepLearningReview, remoteSensingGANsReview, isrgan, tesagan}. Given that much of the recent literature uses this taxonomy, we will adopt the same for this project. We will also investigate pan sharpening, a method used to increase the spatial resolution of remote sensing imagery.

\subsection{Interpolation-based solutions}
Interpolation-based solutions increase image resolution by upscaling the image and then estimating the values of missing pixels. The upscaling process introduces new pixels for which we have no reference value for in the LR image, which the interpolation algorithm estimates by using assuming some relationship between the known pixel values. 

\subsubsection{Nearest neighbour interpolation}
Nearest neighbour interpolation uses the nearest neighbour algorithm to calculate the value of the missing pixels introduced by upscaling~\cite{nnInterpolation}. To find the value of an unknown pixel, we calculate the distance to the surrounding known pixels and then take the closest known pixel value for our unknown pixel. Using nearest neighbour interpolation means no new pixel values are introduced; we always take a pixel from the input image. This results in an upsampled image with sharp edges.

\subsubsection{Polynomial interpolation}
Polynomial interpolation methods assume a polynomial relationship between pixel values. Bilinear interpolation assumes a linear relationship, where unknown pixel values are estimated using a linearly weighted sum of the known surrounding pixel values. Bicubic takes this process a step further, where a cubic relationship is assumed between pixel values. Instead of calculating a linearly weighted sum of surrounding known pixel values, we first calculate the curve between the known pixel values. To get the function of this curve we need to calculate the gradient at each of the known pixels, which itself is calculated using the pixel values surrounding those. Both of these methods introduce pixel values that did not exist in the input, where the change in pixel value is more gradual, creating a smoother image. Polynomial interpolation methods are computationally simple, resulting in quick run times and use in real-time applications~\cite{interpolation}. The crucial limitation of polynomial interpolation is the smoothing effect it has, which removes high-frequency information from the image.~\cite{interpolation}.

\subsubsection{Edge-directed interpolation}
Edge-directed interpolation aims to maintain the sharpness of edges~\cite{interpolation}. In polynomial interpolation unknown pixel values are interpolated based on proximity without considering image contents. In edge-directed interpolation we first attempt to understand the orientation of the edges in the image, and then interpolate along those edges, which maintains them in the output. This difficulty here, however, is that estimation of the edge orientation is not necessarily straightforward from an image processing standpoint, as the edges within natural images are often blurred or noisy to some degree~\cite{interpolation}. There are several edge-directed approaches we can consider. The fusion approach effectively `fuses' the estimations of several edges to provide a good overall estimation of the direction to interpolate along, and is aided by the fusion process being computationally inexpensive~\cite{interpolation}. New edge-directed interpolation (NEDI) takes this a step further by calculating local covariance matrices to determine how pixel values change with direction, where these directions are used to identify edge orientations. This dominant edge orientation is used to interpolate the values of new pixels producing a higher quality result when compared to the simpler fusion technique~\cite{ref}. Soft-decision interpolation determines edge directions similarly to NEDI, but then interpolates with various algorithms to produce several results. The quality of these results are assessed based on edge preservation and artefact avoidance, and a weighting is assigned correspondingly. This weighting is then applied to each of the interpolations to create a combined result that pulls features from various different interpolation algorithms~\cite{ref}.

\subsubsection{Critical analysis of interpolation-based solutions}
There are numerous variations of interpolation-based solutions to the SR problem, however they all share a fundamental flaw. Interpolation is only concerned with reversing the downsampling function in the image degradation process, $D$~\cite{interpolation}. They fail to account for blurring and noise addition which creates suboptimal results when applied to natural imagery that has undergone the entire degradation process. The main benefit of interpolation-based methods is they are generally computationally inexpensive, so for quick or real-time upsampling they may be used instead~\cite{interpolation}.

\subsection{Reconstruction-based solutions}
Reconstruction-based methods improve upon the interpolation methods by considering the blur and noise effects of image degradation, providing a more complete solution and therefore better results. The common theme within this category of solutions is a three-step approach to the problem: interpolation, feature extraction, and reconstruction~\cite{superResRemoteSensingOverview}. This provides a general framework for reconstruction-based methods, but the exact process for each stage is dependent on the specific method~\cite{superResRemoteSensingOverview}.

\subsubsection{Iterative back projection}
Iterative back projection (IBP) iteratively projects features of the LR image onto our SR reconstruction until some stopping criteria is met~\cite{ref}. The first step is to create an initial HR estimation, which can be random initialisation, zero-value initialisation, or the outcome from an interpolation algorithm. We then perform forward projection, where our HR estimation is downsampled, blurred, and noise is introduced to model the image degradation process. The error between the forward projected estimation and true LR image is calculated, and the error itself is upsampled. We then introduce the upsampled error to our HR estimation to produce an SR reconstruction that better resembles ground truth HR. This process is repeated until some stopping criterion is met, for example a maximum number of iterations or a negligible decrease in error. In practice IBP is simple to implement, however can be computationally expensive due to the number of iterations it takes to produce a desirable outcome~\cite{ref}.

\subsubsection{Projection onto convex sets}
Projection onto convex set (POCS) is a generalised mathematical tool used to find solutions that fit a set of predefined constraints. The first step is to define the appropriate constraints for our reconstruction, this might include pixel values, shapes and features we wish to conserve within the image, the required smoothness or sharpness. We represent these constraints as a high-dimensional convex set, where the mathematical definition of a convex set is `for any two points in a convex set, the line joining them remains entirely within the set'. This property allows the projection process to happen, i.e.\ taking our LR image and projecting it to a higher resolution and within our defined constraints. The next step is to guess an initial image, which similar to the previous reconstruction-based methods may be randomly initialised, zero-valued, or the output of some interpolation method. Next, the image initial image estimation is projected onto the constraining convex sets, where the closest point to the projection in the set is determined. This is repeated for all constraining sets, where applying each constraint aligns the image estimation with the HR ground truth image. The entire process is iterated until convergence, i.e.\ negligible change across projections. Whilst POCS can handle a variety of constraints to produce good SR results, it may not always converge, and more crucially, if a constraint cannot be represented as a convex set then it cannot be enforced on the output image.

\subsubsection{Point spread function deconvolution}
A point spread function (PSF) describes how light spreads when captured by imaging hardware, caused by diffraction, imperfections, or sensor failures, which has a blurring effect on the captured image~\cite{ref}. We can artificially produce a blurred image by marching a blurring kernel across it, which performs mathematical operations with the HR image pixels; a process known as convolution. If we know the blurring kernel, we are able reverse this process, or deconvolve, to obtain a reconstruction resembling the HR input. PSF deconvolution uses this idea and deconvolves the LR image with the inverse of some blur kernel that models the PSF that created the LR image. It is very likely that we do not know the exact blur kernel we must use, and it is mostly the case that there is no single blur kernel we can use to perfectly deconvolve an LR image, so the main difficulty becomes deciding the blur kernel we use to model the PSF.\@ PSF deconvolution also suffers from noise amplification, where the blur kernel worsens noise in the LR image.

\subsubsection{Gradient profiling}
Gradient profiling uses specific kernels to calculate the gradients of pixel values, which can then be used guide us in creating an SR reconstruction~\cite{ref}. Firstly we create an HR estimation similar to IBP, where the most common method is to use an interpolation algorithm to upsample the LR image. We then calculate the gradients of both the LR image and our HR estimate through a convolution with a specific kernel. Some examples we can use are the Sobel operator, Prewitt operator, Laplacian operator, or the Canny edge detector. Once we have the gradients for both of the images, we use the gradients of the LR image to transform the gradients of the HR estimation to introduce the sharpness contained within the LR image. The transformed gradients of the HR estimation are then used to reconstruct our HR estimation into our final SR output. Similar to PSF deconvolution, gradient profiling is vulnerable to image noise, and also suffers from expensive computation introduced by the cost of calculating image gradients.

\subsubsection{Critical analysis of reconstruction-based solutions}
The primary success of reconstruction-based solutions is the significant improvement over traditional interpolation-based methods. Reconstruction-based methods account for each stage of the image degradation process so produce less blur and noise. A significant downside of reconstruct-based solutions is their relative computational complexity. Many of the operations are complex optimisation problems, and difficulties converging can result in massive execution durations. Reconstruction-based problems also lose out due to the initial interpolation step, which removes detail from the image~\cite{remoteSensingGANsReview}.

\subsection{Learning-based solutions}
Learning-based solutions aim to learn the function that maps LR imagery to HR imagery. Once the mapping is learned it can be applied to unseen imagery to create an SR reconstruction. The method used to learn the LR to HR mapping varies across approaches.

\subsubsection{Sparse representation}
Sparse representation utilises the sparsity principle to create SR reconstructions. The sparsity principle states that a signal can be sparsely represented using some limited combination of basis functions. For image reconstruction, a dictionary of basis functions is learned from the HR images, where each of the basis function represents some feature of the image, for example, objects, texture, edges, etc. Learning from the HR images is conducted specifically to produce few basis functions to ensure sparsity. Following the dictionary learning process we take the sparse representation of LR imagery in the terms of the learned dictionary. This then allows us to upscale the LR image and apply to learned basis functions to introduce the aforementioned desired features. The final result is an upsampled image that captures the key components of the LR image. Representing the reversal of the image degradation process as a sequence of basis functions allows us to create complex reconstructions, however actually performing the process is often expensive due to the difficult optimisation problems we need to solve to find sparse representations of images.

\subsubsection{Neighbourhood embedding}
Neighbourhood embedding uses learned knowledge of image patches to create SR reconstructions. The first step in the process is to extract patches from the LR image, which are then analysed to retrieve information about what is contained within. This information could be texture, edges, intensity, gradients etc. These patches are then embedded into a higher-dimensional space to discover complex relationships between them. Next the mapping from LR patch to HR patch is learned through training. We can use this mapping to identify which HR patches to use to represent the LR image, which we then stitch together to produce the SR reconstruction. Neighbourhood embedding allows us to preserve detail through the complex relationships learned between patches when embedded in a higher dimensional space. The main downside of neighbourhood is the high computational cost

\subsubsection{Mapping}
The universal goal of learning-based mapping approaches is to learn the function that maps some LR image to a corresponding HR image, which can then be applied to unseen imagery to produce a SR reconstruction. Whilst mapping methods are linked by this common goal, the way that they achieve it can be drastically different~\cite{superResRemoteSensingOverview}. Fernandez-Beltran \etal\ outline some key mapping approaches, such as nearest neighbour mapping, kernel ridge regression, and other regression regularisation approaches. Nearest neighbour mapping reconstructs images using a mapping between LR and HR patches using the nearest neighbour algorithm to identify the most appropriate HR patch to assign, whilst kernel ridge regression performs complex non-linear regression using ridge regularisation to avoid overfitting. Belonging to this set of solutions are deep learning-based approaches. Deep learning uses neural network architectures to approximate the function that defines the relationship between the input data and output data, which is the key idea behind the mapping approach. Deep learning approaches vary significantly therefore we will cover them in a standalone section.

\subsubsection{Critical analysis of learning-based solutions}
Learning-based solutions offer significantly more flexible solutions when compared to the large number of assumptions made to execute reconstruction-based solutions. Learning-based solutions produce results that are more perceptually similar to the HR ground truth images. A large risk with learning-based solutions is the overfitting problem, where the solution fits well to the data it is trained on but has poor generalisation capabilities. Alongside this, large amounts of good-quality data is required to produce good results with learning-based solutions~\cite{superResRemoteSensingOverview}.

\subsection{Deep learning-based solutions}
Neural networks are the building blocks of deep learning. Similar to other mapping approaches, training a neural network on pairs of LR and HR images allows us to learn some generalised function between the two. We can then apply this mapping to unseen LR images to produce an SR reconstruction. This logic is the foundation of deep learning based approaches to the SR reconstruction problem. Deep learning-based approaches have proved so efficient at creating SR reconstructions that many styles of approach exist. Here we overview the several noteworthy deep learning-based solutions to the problem.

\subsubsection{Deep learning background}
Before investigating various deep learning-based solutions, it is useful to overview the components and practices within the field. Deep learning utilises neural networks to learn complex, non-linear relationships from a dataset. A series of connected neurons are iteratively trained with inputs and their corresponding outputs to learn a generalised mapping between the two. The network guesses what the mapping should be, and we measure how wrong the guess is using a scalar value called loss. We can frame neural network training as an optimisation problem, where we tweak the network parameters in such a way so that the loss value is minimised, effectively meaning our estimation of the mapping is as good as possible. This behaviour makes neural networks universal function approximators, allowing us to learn incredibly complex mappings between inputs and outputs.

By arranging our neurons in a grid like structure we can represent image data. Pairing this with a convolutional kernel allows us to learn from local regions within images, opening up a variety of applications. This type of neural network is known as a convolutional neural network (CNN), and lets us learn hierarchical image features, which we can then use to classify images, create lower-dimensional representations, or even create SR reconstructions.

By taking inspiration from game theory, we can model our training process after a zero-sum game. Generative adversarial networks utilise this framework, where two components, a generator and discriminator, play the zero-sum game to iteratively improve their performance. The aim of the generator is to create imagery resembling the training dataset, and the aim of the discriminator is to distinguish between real images from the training data and fake images created by the generator. Both components learn from one another, allowing them to iteratively improve. The final goal is to produce a generator that is so good at creating imagery resembling the training data that it consistently fools the discriminator, at which point we can halt training and use the trained generator to create new imagery

\subsubsection{SRCNN}
Super-resolution Convolutional Neural Network (SRCNN) was proposed in 2015 by Dong \etal\ as a CNN-based solution to the SR reconstruction problem~\cite{srcnn}. The solution architecture consists of three convolutions: patch extraction into a high-dimensional representation, non-linear mapping into HR, and finally image reconstruction. The first convolution extracts features from the LR image, which are then transformed into a high-dimensonal representation, which itself is then mapped into the HR space, ultimately mapping the LR feature maps into HR.\@ The image is then reconstructed from the HR feature maps to give the SR reconstruction. At the time of release, SRCNN outperformed state-of-the-art solutions, however showed very slow convergence times and training~\cite{srcnn}.

\subsubsection{VDSR}
After SRCNN laid the groundwork for CNN-based SR reconstruction solutions, Very Deep Super Resolution (VDSR) was proposed by Kim \etal\ and introduced a significantly deeper solution architecture~\cite{vdsr}. VDSR employs successive feature extracting layers that retrieve the most important features, along with a skip connection between the input image and output of the feature extraction section. The output of the skip connection operation produces the SR reconstructions. VDSR addressed the slow convergence time of SRCNN through the use of a skip connection, as well as producing significantly better results than its predecessor.

\subsubsection{SRGAN}
In 2017, Ledig \etal\ proposed Super-resolution Generative Adversarial Network (SRGAN) as a solution to the SR reconstruction problem~\cite{srgan}. SRGAN was the first GAN-based SR reconstruction solution, utilising the adversarial training process alongside a new loss function to help improve upon results. The generator component of SRGAN, named SRResNet, is itself an SR solution, with the adversarial training process added in to provide further clarity to the image. SRGAN again showed state-of-the-art results when compared to other reconstruction-based methods, and provided the community with the groundwork for more GAN-based solutions.

\subsubsection{ESRGAN}
Wang \etal\ built on SRGAN by modifying the architecture and loss component to bring the SR reconstructions closer to HR ground truth images. Enhanced Super-resolution Generative Adversarial Network (ESRGAN) implements changes threefold~\cite{esrgan}. Firstly, the residual block structure of SRGAN is altered to have a higher capacity whilst also being easier to train. Secondly, the discriminator component is changed to work relativistically, where images are compared instead of being labelled real or fake. Finally, the perceptual loss component is changed so that activations are taken before the VGG19 activations and not after. The result is are SR reconstructions that maintain the sharpness of images and beat the outputs from SRGAN.\@

\subsubsection{Other deep learning-based solutions}
The community remains active, with new solutions being proposed regularly. Here we touch on some newer and noteworthy deep learning-based SR reconstruction. Improved SRGAN (ISRGAN) was suggested by Xiong \etal\ with the aim of addressing the unstable training and overly smooth results of SRGAN.\@ ISRGAN modifies the loss function to support more stable training and alters the network architecture to  avoid the aforementioned smooth results, resulting in better generalisation capabilities~\cite{isrgan,remoteSensingGANsReview}. Texture Enhancement Self Attention Generative Adversarial Network (TE-SAGAN) utilises a self-attention module to ensure texture retention within the SR reconstruction~\cite{tesagan, remoteSensingGANsReview}. Novel Deep Super-resolution Generative Adversarial Network (NDSRGAN) uses numerous dense blocks to avoid distortions in the outputs~\cite{ndsrgan, remoteSensingGANsReview}. 

\subsubsection{Critical analysis of deep learning-based solutions}
The large number of proposed solutions show that deep learning-based approaches are popular methods for solving the SR reconstruction problem. Deep learning-based solutions offer brilliant generalisation capabilities, are more efficient than traditional methods, and are the chosen development direction of the community~\cite{ndsrgan}. Whilst very attractive options, some solutions suffer from extensive complexity, and the solution space itself is highly saturated, making it difficult to offer state-of-the-art results with incremental model improvements. Additionally, deep learning-based solutions, similarly to traditional learning solutions, require a sufficient training set to produce good results, which is not always available~\cite{superResRemoteSensingOverview}.

\subsection{Pansharpening}
Pansharpening is a remote sensing-specific technique for increasing the resolution of imagery. Imaging hardware captures the panchromatic light spectrum as well as visible light. The visible light image is in colour but is LR because of the smaller spectrum. The panchromatic image is greyscale but is HR because over the wider spectrum that extends beyond the visible range. Pansharpening involves fusing the panchromatic image with the visible light imagery. The result is a higher-resolution image, but at the cost of some spectral resolution, caused by the removal of colour detail.

\subsubsection{Critical analysis of additional solutions}
Pansharpening is a simple technique to create HR remote sensing imagery, making it very cost effective, but would not class as a SR reconstruction technique. The process does not aim to reverse the entire image degradation process, so is less effective than reconstruction or learning-based methods. Another crucial flaw is that the panchromatic image is vulnerable to the same issues that the visible light image, such as noise and blur. As soon as the panchromatic image is affected we are not longer able to perform effective pansharpening.

\section{Project goals}
Following the review of the previous solutions to the SR reconstruction problem we can set out our project goals. The aim of this project is to provide a solution to the super-resolution reconstruction problem in remote sensing. Reviewing relevant literature revealed a strong community interest in deep learning-based solutions, with interpolation-based and reconstruction-based solutions being relatively old within the development timeline. It therefore makes sense to consider a deep learning approach to the problem, which require specific requirements, which we reflect in our goals:
\begin{enumerate}
    \item Select an appropriate deep learning-based SR reconstruction method to implement.
    \item Investigate remote sensing datasets for training, validation, and testing of the deep-learning based SR reconstruction method.
    \item Adapt a feature of the deep learning-based method to improve SR reconstruction capabilities.
    \item Train the deep learning-based method using the selected remote sensing dataset.
    \item Evaluate the SR reconstruction capabilities of the adapted method trained on the remote sensing dataset.
    \item Produce better SR reconstructions than at least bicubic interpolation.
\end{enumerate}

<Section requirements. Have I\dots
\begin{itemize}
    \item Explained the problem clearly?
    \item Identified relevant areas for investigation and discussed them in the report under appropriate headings and critically?
    \item Reviewed previous attempts to solve this and similar problems?
    \item Justified any claims made using credible primary or secondary sources?
\end{itemize}
>