\chapter{Introduction and Background Research}\label{chapter1}
\section{Introduction}

Super-resolution reconstruction describes the task of estimating a high-resolution representation of a low-resolution image~\cite{superResOverview}. It is a fundamental problem in the field of computer vision due to the ill-posed nature of the task: there are many `correct' high-resolution representations of a single low-resolution image~\cite{superResChallenges,superResRemoteSensingOverview}. Super-resolution reconstruction is utilised in many domains, including remote sensing, medical imaging, surveillance, astronomy, and more~\cite{superResRemoteSensingChallenges, superResRemoteSensingOverview, superResMedicalImaging, superResSurveillance, superResAstronomy, superResUses}.

Remote sensing is the process of gathering data on an object without making physical contact~\cite{remoteSensing}. A predominant class of remote sensing data is aerial or satellite imagery, where images of Earth are taken from aircraft or satellites~\cite{remoteSensing,remoteSensingImageProcessing}. Such images have proved useful in a variety of areas, including but not limited to urban planning, environmental analysis, land use management, and weather prediction~\cite{remoteSensingUses,remoteSensingGANsReview}.

Remote sensing provides a particularly interesting set of conditions for the super resolution problem due to the constraints introduced by imaging hardware. Optical systems, hardware resolution and atmospheric conditions can result in undesirable image quality, making super-resolution reconstruction techniques attractive alternatives for producing high-resolution imagery~\cite{superResRemoteSensingOverview,superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}.

Super-resolution reconstruction in remote sensing has matured as a field over recent years, with a host of proposed solutions to the problem. Existing methods can be categorised as interpolation-based, reconstruction-based, or learning-based methods, with the deep learning subset of learning methods proving particularly effective~\cite{superResRemoteSensingOverview,srcnn,vdsr}. More recently, the conception of generative deep learning architectures, such as generative adversarial networks, has led to a revived interest in the problem~\cite{srgan,esrgan,isrgan,tesagan,ndsrgan}.

This project explores the super-resolution reconstruction problem in remote sensing in depth, covering the vast solution space via the community-accepted taxonomy of solutions. The main aim of the project is to provide a solution to the super-resolution problem in remote sensing having considered existing solutions, hardware constraints, and project scope.

\section{Background research}\label{sec:background_research}
The following section details our investigation into the super-resolution (SR) reconstruction problem in remote sensing.

\subsection{The problem}\label{subsec:the_problem}
High-resolution (HR) images provide more detail than low-resolution (LR) images~\cite{urbanMapping}. The superior detail offered by HR imagery sanctions the existence of otherwise challenging exercises, including weather prediction, urban mapping, and land cover observation~\cite{urbanMapping, mapping, cloudCover, vegetationMapping}. The most direct way of obtaining HR imagery is to increase the spatial resolution of the imaging hardware, however the amount of light available decreases and noise is introduced into the image~\cite{superResOverview}. An alternative method is to expand chip size to increase capacitance, but this results in undesirable physical inefficiencies and quickly becomes ineffective~\cite{superResOverview}. Increasing image resolution through hardware improvements is paired with high costs, and rapidly changing application requirements do not complement the inflexible nature of such hardware~\cite{remoteSensingGANsReview}. As a result, obtaining HR imagery solely through hardware means becomes difficult.

The natural next step is to look to computational techniques to increase image resolution, leading us to the domain of SR.\@ The SR problem describes the task of generating an HR representation of an LR image. We can formalise the problem as the inverse of the image degradation process, i.e.\ the process that turns some HR image into an LR image~\cite{imageDeg}. The image degradation process is as follows:
\begin{equation}\label{eq:image_deg}
    I_{LR} = D(B(I_{HR})) + \mathcal{N}
\end{equation}
This consists of three one-way functions: blurring ($B$), downsampling ($D$), and finally noise addition ($\mathcal{N}$). As each stage of the process is a one-way function, the SR problem is ill-posed, as multiple SR reconstructions could be perceived as `correct' for a single LR image~\cite{remoteSensingDeepLearningReview}.

Remote sensing describes the process of measuring the properties of objects without making physical contact, typically executed with aircraft or satellites. As a species we rely on remote sensing for a variety of important practices, spanning numerous domains. This includes environmental assessment, global climate change detection, agriculture monitoring, renewable and non-renewable resource observation, meteorology and general weather analysis, land mapping, and military applications~\cite{remoteSensingImageProcessing, remoteSensingUses, remoteSensingGANsReview}.

Many spatial analyses require HR remote sensing imagery to be possible~\cite{urbanMapping}. We measure resolution of remote sensing imagery with spatial resolution, which is the physical area occupied by a single image pixel. In commercially available remote sensing datasets this ranges from 10 centimetres to 1000 metres per pixel, with different applications for different resolutions~\cite{remoteSensingImageProcessing}. In many instances the resolution of imagery available for remote sensing applications is too low, so it becomes very useful to construct HR imagery from LR inputs. Interestingly, we are able to describe equation~\ref{eq:image_deg} in real terms using remote sensing as our context. The blurring function, $B$, is introduced by the optical system capturing the spread of light, the downsampling function, $D$, describes the downsampling process enforced by the resolution of the sensing hardware, and $\mathcal{N}$ describes the addition of noise, usually Gaussian, imposed by atmospheric conditions or sensor failures~\cite{superResRemoteSensingOverview,superResRemoteSensingChallenges, remoteSensingDeepLearningReview, remoteSensingGANsReview}.

It is easy to see that SR has many applications in remote sensing, and that remote sensing imaging hardware is affected by the exact process SR reconstruction strives to reverse. Therefore, the main aim of this project is to investigate and implement a solution to the SR reconstruction problem in remote sensing.

\subsection{Taxonomy of super-resolution solutions}
Before embarking on the investigation into various SR methods we must consider the taxonomy of the solution space. There are numerous classifications we could consider based on various criteria. For example, we may choose to organise solutions based on the domain they operate in, resulting in a classification of either frequency domain or spatial domain~\cite{superResRemoteSensingOverview}. Alternatively, we could choose to split based on the knowledge of parameters we possess beforehand, leading to a classification of blind or non-blind~\cite{superResRemoteSensingOverview}. The complex nature of solutions, along with the increase of hybridised proposals, leads Fernandez-Beltran \etal\ to propose a simplified taxonomy of SR solutions. This taxonomy groups solutions based on their functional properties, and yields three categories: reconstruction-based, learning-based, and hybrid-based~\cite{superResRemoteSensingOverview}. Whilst the taxonomy provided by Fernandez-Beltran \etal\ provides a simpler view than other taxonomies, recent literature ignores the hybrid-based class, and solely suggests the interpolation-based, reconstruction-based, and learning-based taxonomy instead~\cite{remoteSensingDeepLearningReview, remoteSensingGANsReview, isrgan, tesagan}. We will adopt this taxonomy as it falls in line with recent literature.

\subsection{Interpolation-based solutions}
Interpolation-based solutions increase image resolution by upscaling the image and then estimating the values of missing pixels~\cite{interpolation}. The upscaling process introduces new pixels that we have no reference value for in the LR image, which the interpolation algorithm estimates by assuming some relationship between the known pixel values. 

\subsubsection{Nearest neighbour interpolation}
Nearest neighbour interpolation uses the nearest neighbour algorithm to calculate the value of the missing pixels introduced by upscaling~\cite{nnInterpolation}. To find the value of an unknown pixel, we calculate the distance to the surrounding known pixels and then take the closest known pixel value for our unknown pixel. Using nearest neighbour interpolation means no new pixel values are introduced; we always take a pixel from the input image. This results in an upsampled image with sharp edges.

\subsubsection{Polynomial interpolation}
Polynomial interpolation methods assume a polynomial relationship between pixel values~\cite{interpolation}. Bilinear interpolation assumes a linear relationship, where unknown pixel values are estimated using a linearly weighted sum of the known surrounding pixel values. Bicubic takes this process a step further, where a cubic relationship is assumed between pixel values. Instead of calculating a linearly weighted sum of surrounding known pixel values, we first calculate the curve between the known pixel values. To get the function of this curve we need to calculate the gradient at each of the known pixels, which itself is calculated using the pixel values surrounding those. Both of these methods introduce pixel values that did not exist in the input, where the change in pixel value is more gradual, creating a smoother image. Polynomial interpolation methods are computationally simple, resulting in quick run times and use in real-time applications~\cite{interpolation}. The crucial limitation of polynomial interpolation is the smoothing effect it has, which removes high-frequency information from the image.~\cite{interpolation}.

\subsubsection{Edge-directed interpolation}
Edge-directed interpolation aims to maintain the sharpness of edges~\cite{interpolation}. In polynomial interpolation the unknown pixel values are interpolated based on proximity. In edge-directed interpolation we first attempt to understand the orientation of the edges in the image, and then interpolate along those edges, which maintains them in the output. This difficulty here, however, is that estimation of the edge orientation is not necessarily straightforward from an image processing standpoint, as the edges within natural images are often blurred or noisy to some degree~\cite{interpolation}. There are several edge-directed approaches we can consider. The fusion approach effectively `fuses' the estimations of several edges to provide a good overall estimation of the direction to interpolate along, and is aided by the fusion process being computationally inexpensive~\cite{interpolation}. New edge-directed interpolation (NEDI) takes this a step further by calculating local covariance matrices to determine how pixel values change with direction, where these directions are used to identify edge orientations. This dominant edge orientation is used to interpolate the values of new pixels producing a higher quality result when compared to the simpler fusion technique~\cite{interpolation}. Soft-decision interpolation determines edge directions similarly to NEDI, but then interpolates with various algorithms to produce several results~\cite{interpolation}. The quality of these results are assessed based on edge preservation and artefact avoidance, and a weighting is assigned correspondingly. This weighting is then applied to each of the interpolation outputs to create a combined result that pulls features from various different interpolation algorithms.

\subsubsection{Critical analysis of interpolation-based solutions}
There are numerous interpolation-based solutions to the SR problem, however they all share a fundamental flaw: interpolation is only concerned with reversing the downsampling function in the image degradation process~\cite{interpolation}. They fail to account for blurring and noise addition, which creates suboptimal results when applied to natural imagery that has undergone the entire degradation process~\cite{interpolation}. The main benefit of interpolation-based methods is they are generally computationally inexpensive, so for quick or real-time upsampling they may be used instead~\cite{interpolation}.

\subsection{Reconstruction-based solutions}
Reconstruction-based methods improve upon the interpolation methods by considering the blur and noise effects of image degradation, providing a more complete solution and therefore better results. The common theme within this category of solutions is a three-step approach to the problem: interpolation, feature extraction, and reconstruction~\cite{superResRemoteSensingOverview}. This provides a general framework for reconstruction-based methods, but the exact process for each stage is dependent on the specific method~\cite{superResRemoteSensingOverview}.

\subsubsection{Iterative back projection}
Iterative back projection (IBP) iteratively projects features of the LR image onto our SR reconstruction until some stopping criteria is met~\cite{ibp}. The first step is to create an initial HR estimation, which can be random initialisation, zero-value initialisation, or the outcome from an interpolation algorithm. We then perform forward projection, where our HR estimation is downsampled, blurred, and noise is introduced to model the image degradation process. The error between the forward projected estimation and true LR image is calculated, and the error itself is upsampled. We then introduce the upsampled error to our HR estimation to produce an SR reconstruction that better resembles the ground truth HR image.\@ This process is repeated until some stopping criterion is met, for example a maximum number of iterations or a negligible decrease in error. In practice IBP is simple to implement, however artefacts are often introduced into the image through the projection process~\cite{ibp}.

\subsubsection{Projection onto convex sets}
Projection onto convex sets (POCS) is a generalised mathematical tool used to find solutions that fit a set of predefined constraints~\cite{pocs,pocsEndoscopy}. The first step is to define the appropriate constraints for our reconstruction. This might include pixel values, image smoothness or sharpness, or shapes and features we wish to conserve. We represent these constraints as a high-dimensional convex set, where the mathematical definition of a convex set is: for any two points in a convex set, the line joining them remains entirely within the set~\cite{pocsEndoscopy}. This property allows the projection process to happen, i.e.\ taking our LR image and projecting it to a higher resolution whilst adhering to our defined constraints. The next step is to guess an initial image, which similar to IBP may be randomly initialised, zero-valued, or the output of some interpolation method. Next, the initial image estimation is projected onto the constraining convex sets, where the closest point to the projection in the set is determined. This is repeated for all constraining sets, where applying each constraint aligns the image estimation with the HR ground truth image. The entire process is iterated until convergence. POCS is very useful as we are able to represent prior knowledge about the image as our constraints, improving the overall solution, however, it is often not clear when is best to stop the iterative process to achieve the best results~\cite{pocsEndoscopy}.

\subsubsection{Point spread function deconvolution}
A point spread function (PSF) describes how light spreads when captured by imaging lenses, which has a blurring effect on the output~\cite{psfDeconv}. We can artificially produce a blurred image by marching a blurring kernel across it, which performs mathematical operations with the HR image pixels, a process known as convolution~\cite{convBlur}. If we know the blurring kernel, we can reverse this process, or deconvolve, to obtain a reconstruction resembling the HR input. PSF deconvolution uses this idea and deconvolves the LR image with the inverse of some blur kernel that models the PSF that created the LR image. We will never know the exact blur kernel we must use to perfectly deconvolve an image, so the main difficulty becomes deciding how we model the PSF~\cite{pocs}.

\subsubsection{Critical analysis of reconstruction-based solutions}
The primary success of reconstruction-based solutions is the significant improvement over traditional interpolation-based methods~\cite{interpolation}. Reconstruction-based methods account for each stage of the image degradation process, so produce less blur and noise~\cite{interpolation}. A significant downside of reconstruction-based solutions is that they lose out due to the initial interpolation step, which removes some level of detail from the image~\cite{remoteSensingGANsReview}.

\subsection{Learning-based solutions}
Learning-based solutions aim to learn how to map LR imagery to HR imagery~\cite{superResRemoteSensingOverview}. Once the mapping is learned it can be applied to unseen imagery to create an SR reconstruction. The method used to learn the LR to HR mapping varies across approaches.

\subsubsection{Sparse representation}
Sparse representation utilises the sparsity principle to create SR reconstructions~\cite{superResRemoteSensingOverview}. The sparsity principle states that a signal can be sparsely represented using some limited combination of basis functions. For image reconstruction, a dictionary of basis functions is learned from the HR images, where each of the basis functions represents some feature of the image, for example, objects, texture, edges, etc. Learning from the HR images is conducted specifically to produce few basis functions to ensure sparsity. The sparse representation of LR imagery is created in the terms of the learned dictionary. This then allows us to upscale the LR image and apply the learned basis functions to introduce the aforementioned desired features. The final result is an upsampled image that captures the key components of the LR image. Representing the reversal of the image degradation process as a sequence of basis functions allows us to create complex reconstructions that are robust against noise, however actually performing the process is often expensive due to the difficult optimisation problems we need to solve to find sparse representations of images~\cite{sparseRep}.

\subsubsection{Neighbourhood embedding}
Neighbourhood embedding uses learned knowledge of image patches to create SR reconstructions~\cite{neighbourhoodEmbedding}. The first step in the process is to extract patches from the LR image, which are then analysed to retrieve information about what is contained within. This information could be texture, edges, intensity, gradients etc. These patches are then embedded into a higher-dimensional space to discover complex relationships between them. Next the mapping from LR patch to HR patch is learned through training. We can use this mapping to identify which HR patches to use to represent the LR image, which we then stitch together to produce the SR reconstruction. Neighbourhood embedding requires fewer training examples than other methods due to its superior generalisation capabilities, introduced by the neighbourhood learning functionality~\cite{neighbourhoodEmbedding}

\subsubsection{Mapping}
The universal goal of learning-based mapping approaches is to learn the function that maps some LR image to a corresponding HR image, which can then be applied to unseen imagery to produce a SR reconstruction~\cite{superResRemoteSensingOverview}. Fernandez-Beltran \etal\ outline some key mapping approaches, including nearest neighbour mapping, which reconstructs images using a mapping between LR and HR patches, using the nearest neighbour algorithm to identify the most appropriate HR patch to assign. Kernel ridge regression performs complex non-linear regression using ridge regularisation to avoid overfitting. Included in this set of solutions are deep learning-based approaches. Deep learning uses neural network architectures to approximate the function that defines the relationship between the input data and output data, which is the key idea behind the mapping approach. Deep learning approaches vary significantly, therefore we will cover them in a standalone section.

\subsubsection{Critical analysis of learning-based solutions}
Learning-based solutions are more flexible than reconstruction-based solutions, as the image degradation process is learned from the training data and not assumed through priors~\cite{neighbourhoodEmbedding}. A risk with learning-based solutions is the overfitting problem, where the solution fits well to the data it is trained on but has poor generalisation capabilities~\cite{overfitting}. Alongside this, large amounts of good-quality data is required to produce good results with learning-based solutions~\cite{superResRemoteSensingOverview}.

\subsection{Deep learning-based solutions}
Here we overview several noteworthy deep learning-based solutions to the problem.

\subsubsection{Deep learning background}
Before investigating various deep learning-based solutions, it is useful to overview the components and practices within the field. Deep learning utilises neural networks to learn complex, non-linear relationships from a dataset~\cite{nn}. A series of connected nodes, called neurons, are iteratively trained with inputs and their corresponding outputs to learn a generalised mapping between the two. The network guesses what the mapping should be, and we measure how wrong the guess is using a scalar value called loss. We can frame neural network training as an optimisation problem, where we tweak the network parameters in such a way so that the loss value is minimised, effectively making our estimation of the mapping as good as possible. This behaviour makes neural networks universal function approximators, allowing us to learn incredibly complex mappings between inputs and outputs~\cite{nn}.

By arranging our neurons in a grid like structure we can represent image data~\cite{imageNet}. Pairing this with a convolutional kernel allows us to learn from local regions within images, opening up a variety of applications~\cite{imageNet}. This type of neural network is known as a convolutional neural network (CNN), and lets us learn hierarchical image features, which we can then use to classify images, extract features, or even create SR reconstructions~\cite{vgg19,imageNet,srcnn}.

By taking inspiration from game theory, we can model our training process after a zero-sum game~\cite{gan}. Generative adversarial networks (GANs) utilise this framework, where two components, a generator and discriminator, play the zero-sum game to iteratively improve their performance~\cite{gan}. The aim of the generator is to create imagery resembling the training dataset, and the aim of the discriminator is to distinguish between real images from the training data and fake images created by the generator. Both components learn from one another, allowing them to iteratively improve. The final goal is to produce a generator that is so good at creating imagery resembling the training data that it consistently fools the discriminator, at which point we can halt training and use the trained generator to create new imagery~\cite{gan}.

\subsubsection{SRCNN}
Super-resolution Convolutional Neural Network (SRCNN) was proposed in 2015 by Dong \etal\ as a CNN-based solution to the SR reconstruction problem~\cite{srcnn}. The solution architecture consists of three convolutions: patch extraction into a high-dimensional representation, non-linear mapping into HR, and finally image reconstruction. The first convolution extracts features from the LR image, which are transformed into a high-dimensional representation, which itself is then mapped into the HR space, ultimately mapping the LR feature maps into HR.\@ The image is then reconstructed from the HR feature maps to give the reconstruction. Initially, SRCNN outperformed state-of-the-art solutions, but showed very slow convergence and training~\cite{srcnn, vdsr}.

\subsubsection{VDSR}
After SRCNN laid the groundwork for CNN-based SR reconstruction solutions, Very Deep Super Resolution (VDSR) was proposed by Kim \etal\ and introduced a significantly deeper solution architecture~\cite{vdsr}. VDSR employs successive feature extracting layers that retrieve the most important features, along with a skip connection between the input image and output of the feature extraction section. The output of the skip connection addition produces the SR reconstruction. VDSR addressed the slow convergence time of SRCNN through the use of a skip connection, as well as producing significantly better results than its predecessor~\cite{vdsr}.

\subsubsection{SRGAN}
In 2017, Ledig \etal\ proposed Super-resolution Generative Adversarial Network (SRGAN) as a solution to the SR reconstruction problem~\cite{srgan}. SRGAN was the first GAN-based SR reconstruction solution, utilising the adversarial training process alongside a new loss function to help improve upon results. The generator component of SRGAN, named SRResNet, is itself an SR solution, with the adversarial training process added in to provide further clarity to the image. SRGAN again showed state-of-the-art results when compared to other methods, and provided the community with the groundwork for more GAN-based solutions~\cite{srgan}.

\subsubsection{ESRGAN}
Wang \etal\ built on SRGAN by modifying the architecture and loss component to bring the SR reconstructions closer to HR ground truth images. Enhanced Super-resolution Generative Adversarial Network (ESRGAN) implements three changes~\cite{esrgan}. Firstly, the residual block structure of SRGAN is altered to have a higher capacity, whilst also being easier to train. Secondly, the discriminator component is changed to work relativistically, where images are compared instead of being labelled real or fake. Finally, the perceptual loss component is changed so that feature maps are taken before activation and not after. The result is SR reconstructions that maintain the sharpness of images and beat the outputs from SRGAN~\cite{esrgan}.

\subsubsection{Other deep learning-based solutions}
The community remains active, with new solutions being proposed regularly. Here we touch on some noteworthy newer deep learning-based SR reconstruction solutions. Improved SRGAN (ISRGAN) was suggested by Xiong \etal\ with the aim of addressing the unstable training and overly smooth results of SRGAN.\@ ISRGAN modifies the loss function to support more stable training and alters the network architecture to  avoid the aforementioned smooth results, resulting in better generalisation capabilities~\cite{isrgan,remoteSensingGANsReview}. Texture Enhancement Self Attention Generative Adversarial Network (TE-SAGAN) utilises a self-attention module to ensure texture retention within the SR reconstruction~\cite{tesagan, remoteSensingGANsReview}. Novel Deep Super-resolution Generative Adversarial Network (NDSRGAN) uses numerous dense blocks to avoid distortions in its outputs~\cite{ndsrgan, remoteSensingGANsReview}. 

\subsubsection{Critical analysis of deep learning-based solutions}
The large number of proposed solutions show that deep learning-based approaches are popular methods for solving the SR reconstruction problem. Deep learning-based solutions offer brilliant generalisation capabilities, are more efficient than traditional methods, and are the chosen development direction of the community~\cite{ndsrgan}. Whilst very attractive options, some solutions suffer from extensive complexity, and the solution space itself is highly saturated, making it difficult to offer state-of-the-art results with incremental model improvements. Additionally, deep learning-based solutions, similarly to traditional learning solutions, require a sufficient training set to produce good results, which is not always available~\cite{superResRemoteSensingOverview}.

\section{Project goals}
Following the review of the previous solutions to the SR reconstruction problem we can set out our project goals. The aim of this project is to provide a solution to the SR reconstruction problem in remote sensing. Reviewing relevant literature revealed a strong community interest in deep learning-based solutions, with interpolation-based and reconstruction-based solutions being relatively old within the development timeline. It therefore makes sense to consider a deep learning approach to the problem, which has specific requirements that we reflect in our selection of goals:
\begin{enumerate}
    \item Select an appropriate deep learning-based SR reconstruction method to implement.
    \item Investigate remote sensing datasets for training, validation, and testing of the deep-learning based SR reconstruction method.
    \item Adapt a feature of the deep learning-based method to improve SR reconstruction capabilities.
    \item Train the deep learning-based method using the selected remote sensing dataset.
    \item Evaluate the SR reconstruction capabilities of the adapted method trained on the remote sensing dataset.
    \item Produce better SR reconstructions than at least bicubic interpolation.
\end{enumerate}